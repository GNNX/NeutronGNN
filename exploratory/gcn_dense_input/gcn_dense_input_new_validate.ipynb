{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Network\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.nn import GCNConv, ChebConv\n",
    "from torch_geometric.nn import global_max_pool\n",
    "\n",
    "# Data\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, Sampler\n",
    "\n",
    "from torch_geometric.data import Batch, Data, Dataset, DataLoader\n",
    "\n",
    "# General\n",
    "import numpy as np\n",
    "\n",
    "# Util\n",
    "import os.path as osp\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate fake indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length = 2937\n",
    "# splits = 10\n",
    "# random_shuffle = np.random.permutation(length)\n",
    "# validation_indicies = random_shuffle[:length//splits]\n",
    "# test_indicies = random_shuffle[length//splits:2*length//splits]\n",
    "# train_indicies = random_shuffle[2*length//splits:]\n",
    "\n",
    "# with open(\"train_indicies.txt\", 'w') as f:\n",
    "#     f.writelines([\"{}\\n\".format(i) for i in train_indicies])\n",
    "    \n",
    "# with open(\"validation_indicies.txt\", 'w') as f:\n",
    "#     f.writelines([\"{}\\n\".format(i) for i in validation_indicies])\n",
    "\n",
    "# with open(\"test_indicies.txt\", 'w') as f:\n",
    "#     f.writelines([\"{}\\n\".format(i) for i in test_indicies])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Port from https://github.com/tkarras/progressive_growing_of_gans/blob/master/config.py\n",
    "class EasyDict(dict):\n",
    "    def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs)\n",
    "    def __getattr__(self, name): return self[name]\n",
    "    def __setattr__(self, name, value): self[name] = value\n",
    "    def __delattr__(self, name): del self[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = EasyDict()\n",
    "\n",
    "config.model_name = \"gcn_kipf\"\n",
    "\n",
    "config.data_path = \"/app/test_data/IWCDmPMT_4pi_fulltank_test_graphnet.h5\"\n",
    "config.train_indices_file = \"/app/test_data/IWCDmPMT_4pi_fulltank_test_splits/train.txt\"\n",
    "config.val_indices_file = \"/app/test_data/IWCDmPMT_4pi_fulltank_test_splits/val.txt\"\n",
    "config.test_indices_file = \"/app/test_data/IWCDmPMT_4pi_fulltank_test_splits/test.txt\"\n",
    "config.edge_index_pickle = \"/app/GraphNets/metadata/edges_dict.pkl\"\n",
    "\n",
    "config.dump_path = \"/app/GraphNets/dump/gcn\"\n",
    "\n",
    "config.num_data_workers = 0 # Sometime crashes if we do multiprocessing\n",
    "config.device = 'gpu'\n",
    "config.gpu_list = [0]\n",
    "\n",
    "config.batch_size = 32 # 256\n",
    "config.validate_batch_size = 32\n",
    "config.lr=0.01\n",
    "config.weight_decay=5e-4\n",
    "\n",
    "config.epochs = 1\n",
    "config.report_interval = 10 # 100\n",
    "config.num_val_batches  = 32\n",
    "config.valid_interval   = 100 # 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WCH5Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset storing image-like data from Water Cherenkov detector\n",
    "    memory-maps the detector data from hdf5 file\n",
    "    The detector data must be uncompresses and unchunked\n",
    "    labels are loaded into memory outright\n",
    "    No other data is currently loaded \n",
    "    \"\"\"\n",
    "\n",
    "    # Override the default implementation\n",
    "    def _download(self):\n",
    "        pass\n",
    "    \n",
    "    def _process(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def __init__(self, path, train_indices_file, val_indices_file, test_indices_file, \n",
    "                 edge_index_pickle, nodes=15808,\n",
    "                 transform=None, pre_transform=None, pre_filter=None, \n",
    "                 use_node_attr=False, use_edge_attr=False, cleaned=False):\n",
    "\n",
    "        super(WCH5Dataset, self).__init__(\"\", transform, pre_transform,\n",
    "                                        pre_filter)\n",
    "        \n",
    "        f=h5py.File(path,'r')\n",
    "        hdf5_event_data = f[\"event_data\"]\n",
    "        hdf5_labels=f[\"labels\"]\n",
    "\n",
    "        assert hdf5_event_data.shape[0] == hdf5_labels.shape[0]\n",
    "\n",
    "        event_data_shape = hdf5_event_data.shape\n",
    "        event_data_offset = hdf5_event_data.id.get_offset()\n",
    "        event_data_dtype = hdf5_event_data.dtype\n",
    "\n",
    "        #this creates a memory map - i.e. events are not loaded in memory here\n",
    "        #only on get_item\n",
    "        self.event_data = np.memmap(path, mode='r', shape=event_data_shape, \n",
    "                                    offset=event_data_offset, dtype=event_data_dtype)\n",
    "        \n",
    "        #this will fit easily in memory even for huge datasets\n",
    "        self.labels = np.array(hdf5_labels)\n",
    "        self.nodes = nodes\n",
    "        self.load_edges(edge_index_pickle)\n",
    "        \n",
    "        self.transform=transform\n",
    "        \n",
    "        #the section below handles the subset\n",
    "        #(for reduced dataset training tests)\n",
    "        #as well as shuffling and train/test/validation splits\n",
    "            \n",
    "        self.train_indices = self.load_indicies(train_indices_file)\n",
    "        self.val_indices = self.load_indicies(val_indices_file)\n",
    "        self.test_indices = self.load_indicies(test_indices_file)\n",
    "    \n",
    "    def load_indicies(self, indicies_file):\n",
    "        with open(indicies_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        # indicies = [int(l.strip()) for l in lines if not l.isspace()]\n",
    "        indicies = [int(l.strip()) for l in lines]\n",
    "        return indicies\n",
    "    \n",
    "    def load_edges(self, edge_index_pickle):\n",
    "        edge_index = torch.zeros([self.nodes, self.nodes], dtype=torch.int64)\n",
    "\n",
    "        with open(edge_index_pickle, 'rb') as f:\n",
    "            edges = pickle.load(f)\n",
    "\n",
    "            for k,vs in edges.items():\n",
    "                for v in vs:\n",
    "                    edge_index[k,v] = 1\n",
    "\n",
    "        self.edge_index=edge_index.to_sparse()._indices()\n",
    "    \n",
    "    def get(self, idx):\n",
    "        x = torch.from_numpy(self.event_data[idx])\n",
    "        y = torch.tensor([self.labels[idx]], dtype=torch.int64)\n",
    "\n",
    "        return Data(x=x, y=y, edge_index=self.edge_index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(path, train_indices_file, val_indices_file, test_indices_file, edges_dict_pickle, batch_size, workers):\n",
    "    \n",
    "    dataset = WCH5Dataset(path, train_indices_file, val_indices_file, test_indices_file, edges_dict_pickle)\n",
    "                          \n",
    "    train_loader=DataLoader(dataset, batch_size=batch_size, num_workers=workers,\n",
    "                            pin_memory=True, sampler=SubsetRandomSampler(dataset.train_indices))\n",
    "\n",
    "    val_loader=DataLoader(dataset, batch_size=batch_size, num_workers=workers,\n",
    "                            pin_memory=True, sampler=SubsetRandomSampler(dataset.val_indices))\n",
    "\n",
    "    return train_loader, val_loader, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubsetSequentialSampler(Sampler):\n",
    "    r\"\"\"Samples elements randomly from a given list of indices, without replacement.\n",
    "\n",
    "    Arguments:\n",
    "        indices (sequence): a sequence of indices\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, indices):\n",
    "        self.indices = indices\n",
    "\n",
    "    def __iter__(self):\n",
    "#         return (i for i in self.indices)\n",
    "        return iter(self.indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(2, 8, cached=False)\n",
    "        self.conv2 = GCNConv(8, 32, cached=False)\n",
    "        self.conv3 = GCNConv(32, 128, cached=False)\n",
    "        self.linear = Linear(128, 3)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, edge_index, batch_index = batch.x, batch.edge_index, batch.batch\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = global_max_pool(x, batch_index)\n",
    "        x = self.linear(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python standard imports\n",
    "from abc import ABC, abstractmethod\n",
    "from time import strftime\n",
    "from os import stat, mkdir\n",
    "from math import floor, ceil\n",
    "\n",
    "# PyTorch imports\n",
    "from torch import device, load, save\n",
    "from torch.nn import DataParallel\n",
    "from torch.cuda import is_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Engine(ABC):\n",
    "\n",
    "    def __init__(self, model, config):\n",
    "        super().__init__()\n",
    "\n",
    "        # Engine attributes\n",
    "        self.model=model\n",
    "        self.config=config\n",
    "\n",
    "        # Determine the device to be used for model training and inference\n",
    "        if (config.device == 'gpu') and config.gpu_list:\n",
    "            print(\"Requesting GPUs. GPU list : \" + str(config.gpu_list))\n",
    "            self.devids=[\"cuda:{0}\".format(x) for x in config.gpu_list]\n",
    "            print(\"Main GPU : \" + self.devids[0])\n",
    "\n",
    "            if is_available():\n",
    "                self.device=device(self.devids[0])\n",
    "                if len(self.devids) > 1:\n",
    "                    print(\"Using DataParallel on these devices: {}\".format(self.devids))\n",
    "                    self.model=DataParallel(self.model, device_ids=config.gpu_list, dim=0)\n",
    "                print(\"CUDA is available\")\n",
    "            else:\n",
    "                self.device=device(\"cpu\")\n",
    "                print(\"CUDA is not available\")\n",
    "        else:\n",
    "            print(\"Unable to use GPU\")\n",
    "            self.device=device(\"cpu\")\n",
    "\n",
    "        # Send the model to the selected device\n",
    "        self.model = DataParallel(self.model) if len(self.devids) > 1 else self.model # Changed\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Setup the parameters tp save given the model type\n",
    "        if type(self.model) == DataParallel:\n",
    "            self.model_accs=self.model.module\n",
    "        else:\n",
    "            self.model_accs=self.model\n",
    "  \n",
    "        # Create the dataset object\n",
    "        out = get_loaders(config.data_path, \n",
    "                      config.train_indices_file, config.val_indices_file, config.test_indices_file,   # Changed\n",
    "                      config.edge_index_pickle, config.batch_size, config.num_data_workers)\n",
    "\n",
    "        self.train_loader, self.val_loader, self.dataset = out\n",
    "        # Define the variant dependent attributes\n",
    "        self.criterion=None\n",
    "\n",
    "        # Create the directory for saving the log and dump files\n",
    "        self.dirpath=config.dump_path + strftime(\"%Y%m%d_%H%M%S\") + \"/\"\n",
    "        try:\n",
    "            stat(self.dirpath)\n",
    "        except:\n",
    "            print(\"Creating a directory for run dump at : {}\".format(self.dirpath))\n",
    "            mkdir(self.dirpath)\n",
    "\n",
    "        # Logging attributes\n",
    "        self.train_log=CSVData(self.dirpath + \"log_train.csv\")\n",
    "        self.val_log=CSVData(self.dirpath + \"log_val.csv\")\n",
    "\n",
    "#         # Save a copy of the config in the dump path\n",
    "#         save_config(self.config, self.dirpath + \"config_file.ini\")   # changed\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, data, mode):\n",
    "        \"\"\"Forward pass using self.data as input.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, predict, expected):\n",
    "        \"\"\"Backward pass using the loss computed for a mini-batch.\"\"\"\n",
    "        self.optimizer.zero_grad()  # Reset gradient accumulation\n",
    "        loss = self.criterion(predict, expected)\n",
    "        loss.backward()# Propagate the loss backwards\n",
    "        self.optimizer.step()       # Update the optimizer parameters         \n",
    "        \n",
    "        return loss\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self):\n",
    "        \"\"\"Training loop over the entire dataset for a given number of epochs.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save_state(self, mode=\"latest\"):\n",
    "        \"\"\"Save the model parameters in a file.\n",
    "        \n",
    "        Args :\n",
    "        mode -- one of \"latest\", \"best\" to differentiate\n",
    "                the latest model from the model with the\n",
    "                lowest loss on the validation subset (default \"latest\")\n",
    "        \"\"\"\n",
    "        path=self.dirpath + self.config.model_name + \"_\" + mode + \".pth\"\n",
    "\n",
    "        # Extract modules from the model dict and add to start_dict \n",
    "        modules=list(self.model_accs._modules.keys())\n",
    "        state_dict={module: getattr(self.model_accs, module).state_dict() for module in modules}\n",
    "\n",
    "        # Save the model parameter dict\n",
    "        save(state_dict, path)\n",
    "\n",
    "    def load_state(self, path):\n",
    "        \"\"\"Load the model parameters from a file.\n",
    "        \n",
    "        Args :\n",
    "        path -- absolute path to the .pth file containing the dictionary\n",
    "        with the model parameters to load from\n",
    "        \"\"\"\n",
    "        # Open a file in read-binary mode\n",
    "        with open(path, 'rb') as f:\n",
    "\n",
    "            # Interpret the file using torch.load()\n",
    "            checkpoint=load(f, map_location=self.device)\n",
    "\n",
    "            print(\"Loading weights from file : {0}\".format(path))\n",
    "\n",
    "            local_module_keys=list(self.model_accs._modules.keys())\n",
    "            for module in checkpoint.keys():\n",
    "                if module in local_module_keys:\n",
    "                    print(\"Loading weights for module = \", module)\n",
    "                    getattr(self.model_accs, module).load_state_dict(checkpoint[module])\n",
    "                    \n",
    "        self.model.to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python standard imports\n",
    "from sys import stdout\n",
    "from math import floor, ceil\n",
    "from time import strftime, localtime\n",
    "\n",
    "# PyTorch imports\n",
    "from torch.optim import Adam\n",
    "\n",
    "# WatChMaL imports\n",
    "# from training_utils.engine import Engine\n",
    "# from plot_utils.notebook_utils import CSVData\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVData:\n",
    "\n",
    "    def __init__(self,fout):\n",
    "        self.name  = fout\n",
    "        self._fout = None\n",
    "        self._str  = None\n",
    "        self._dict = {}\n",
    "\n",
    "    def record(self, keys, vals):\n",
    "        for i, key in enumerate(keys):\n",
    "            self._dict[key] = vals[i]\n",
    "\n",
    "    def write(self):\n",
    "        if self._str is None:\n",
    "            self._fout=open(self.name,'w')\n",
    "            self._str=''\n",
    "            for i,key in enumerate(self._dict.keys()):\n",
    "                if i:\n",
    "                    self._fout.write(',')\n",
    "                    self._str += ','\n",
    "                self._fout.write(key)\n",
    "                self._str+='{:f}'\n",
    "            self._fout.write('\\n')\n",
    "            self._str+='\\n'\n",
    "\n",
    "        self._fout.write(self._str.format(*(self._dict.values())))\n",
    "        self.flush()\n",
    "\n",
    "    def flush(self):\n",
    "        if self._fout: self._fout.flush()\n",
    "\n",
    "    def close(self):\n",
    "        if self._str is not None:\n",
    "            self._fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EngineGraph(Engine):\n",
    "\n",
    "    def __init__(self, model, config):\n",
    "        super().__init__(model, config)\n",
    "        self.criterion=F.nll_loss\n",
    "        self.optimizer=Adam(self.model_accs.parameters(), lr=config.lr)\n",
    "        \n",
    "        self.keys = ['iteration', 'epoch', 'loss', 'acc']\n",
    "\n",
    "    def forward(self, data, mode=\"train\"):\n",
    "        \"\"\"Overrides the forward abstract method in Engine.py.\n",
    "        \n",
    "        Args:\n",
    "        mode -- One of 'train', 'validation' \n",
    "        \"\"\"\n",
    "\n",
    "        # Set the correct grad_mode given the mode\n",
    "        if mode == \"train\":\n",
    "            self.model.train()\n",
    "        elif mode in [\"validation\"]:\n",
    "            self.model.eval()\n",
    "\n",
    "        return self.model(data)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Overrides the train method in Engine.py.\n",
    "        \n",
    "        Args: None\n",
    "        \"\"\"\n",
    "        \n",
    "        epochs          = self.config.epochs\n",
    "        report_interval = self.config.report_interval\n",
    "        valid_interval  = self.config.valid_interval\n",
    "        num_val_batches = self.config.num_val_batches\n",
    "\n",
    "        # Initialize counters\n",
    "        epoch=0.\n",
    "        iteration=0\n",
    "\n",
    "        # Parameter to upadte when saving the best model\n",
    "        best_loss=1000000.\n",
    "\n",
    "        val_iter = iter(self.val_loader)\n",
    "        \n",
    "        # Global training loop for multiple epochs\n",
    "        while (floor(epoch) < epochs):\n",
    "\n",
    "            print('Epoch', np.round(epoch).astype(np.int),\n",
    "                  'Starting @', strftime(\"%Y-%m-%d %H:%M:%S\", localtime()))\n",
    "\n",
    "            # Local training loop for a single epoch\n",
    "            for data in self.train_loader:\n",
    "                data = data.to(self.device)\n",
    "\n",
    "                # Update the epoch and iteration\n",
    "                epoch+=1. / len(self.train_loader)\n",
    "                iteration += 1\n",
    "                \n",
    "                # Do a forward pass using data = self.data\n",
    "                res=self.forward(data, mode=\"train\")\n",
    "\n",
    "                # Do a backward pass using loss = self.loss\n",
    "                loss = self.backward(res, data.y)\n",
    "\n",
    "                acc = res.argmax(1).eq(data.y).sum().item()/data.y.shape[0]\n",
    "                \n",
    "                # Record the metrics for the mini-batch in the log\n",
    "                self.train_log.record(self.keys, [iteration, epoch, loss, acc])\n",
    "                self.train_log.write()\n",
    "\n",
    "                # Print the metrics at given intervals\n",
    "                if iteration % report_interval == 0:\n",
    "                    print(\"... Iteration %d ... Epoch %1.2f ... Loss %1.3f ... Acc %1.3f\"\n",
    "                          % (iteration, epoch, loss, acc))\n",
    "\n",
    "                # Run validation on given intervals\n",
    "                if iteration % valid_interval == 0:\n",
    "                    with torch.no_grad():\n",
    "                        val_loss=0.\n",
    "                        val_acc=0.\n",
    "\n",
    "                        for val_batch in range(num_val_batches):\n",
    "\n",
    "                            try:\n",
    "                                data=next(val_iter)\n",
    "                            except StopIteration:\n",
    "                                val_iter=iter(self.val_loader)\n",
    "                                data=next(val_iter)\n",
    "                            data = data.to(self.device)\n",
    "\n",
    "                            # Extract the event data from the input data tuple\n",
    "                            res=self.forward(data, mode=\"validation\")\n",
    "                            acc = res.argmax(1).eq(data.y).sum().item()/data.y.shape[0]\n",
    "\n",
    "                            val_loss+=self.criterion(res, data.y)\n",
    "                            val_acc+=acc\n",
    "\n",
    "                        val_loss /= num_val_batches\n",
    "                        val_acc /= num_val_batches\n",
    "\n",
    "                        # Record the validation stats to the csv\n",
    "                        self.val_log.record(self.keys, [iteration, epoch, loss, acc])\n",
    "                        self.val_log.write()\n",
    "\n",
    "                        # Save the best model\n",
    "                        if val_loss < best_loss:\n",
    "                            self.save_state(mode=\"best\")\n",
    "                            best_loss = val_loss\n",
    "\n",
    "                        # Save the latest model\n",
    "                        self.save_state(mode=\"latest\")\n",
    "                    \n",
    "\n",
    "        self.val_log.close()\n",
    "        self.train_log.close()\n",
    "\n",
    "    def validate(self, subset):\n",
    "        \"\"\"Overrides the validate method in Engine.py.\n",
    "        \n",
    "        Args:\n",
    "        subset          -- One of 'train', 'validation', 'test' to select the subset to perform validation on\n",
    "        \"\"\"\n",
    "        # Print start message\n",
    "        if subset == \"train\":\n",
    "            message=\"Validating model on the train set\"\n",
    "        elif subset == \"validation\":\n",
    "            message=\"Validating model on the validation set\"\n",
    "        elif subset == \"test\":\n",
    "            message=\"Validating model on the test set\"\n",
    "        else:\n",
    "            print(\"validate() : arg subset has to be one of train, validation, test\")\n",
    "            return None\n",
    "\n",
    "        print(message)\n",
    "        \n",
    "        # Setup the CSV file for logging the output, path to save the actual and reconstructed events, dataloader iterator\n",
    "        if subset == \"train\":\n",
    "            self.log=CSVData(self.dirpath + \"train_validation_log.csv\")\n",
    "            validate_indices = self.dataset.train_indices\n",
    "        elif subset == \"validation\":\n",
    "            self.log=CSVData(self.dirpath + \"valid_validation_log.csv\")\n",
    "            validate_indices = self.dataset.val_indices\n",
    "        else:\n",
    "            self.log=CSVData(self.dirpath + \"test_validation_log.csv\")\n",
    "            validate_indices = self.dataset.test_indices\n",
    "\n",
    "        data_iter = DataLoader(self.dataset, batch_size=self.config.validate_batch_size, \n",
    "                               num_workers=self.config.num_data_workers,\n",
    "                               pin_memory=True, sampler=SubsetSequentialSampler(validate_indices))\n",
    "\n",
    "        headers = [\"index\", \"label\", \"pred\"]\n",
    "        for i in range(max(self.dataset.labels)+1):\n",
    "            headers.append(\"pred_val{}\".format(i))\n",
    "\n",
    "        avg_loss = 0\n",
    "        avg_acc = 0\n",
    "        indices_iter = iter(validate_indices)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for iteration, data in enumerate(data_iter):\n",
    "\n",
    "                gpu_data = data.to(self.device)\n",
    "\n",
    "                stdout.write(\"Iteration : {}, Progress {} \\n\".format(iteration, iteration/len(data_iter)))\n",
    "                res=self.forward(gpu_data, mode=\"validation\")\n",
    "\n",
    "                acc = res.argmax(1).eq(gpu_data.y).sum().item()\n",
    "                loss = self.criterion(res, gpu_data.y) * data.y.shape[0]\n",
    "                avg_acc += acc\n",
    "                avg_loss += loss\n",
    "\n",
    "                # Log/Report\n",
    "                for label, pred, preds in zip(data.y.tolist(), res.argmax(1).tolist(), res.exp().tolist()):\n",
    "                    output = [next(indices_iter), label, pred]\n",
    "                    for p in preds:\n",
    "                        output.append(p)\n",
    "                    self.log.record(headers, output)\n",
    "                    self.log.write()\n",
    "\n",
    "        avg_acc/=len(validate_indices)\n",
    "        avg_loss/=len(validate_indices)\n",
    "\n",
    "        stdout.write(\"Overall acc : {}, Overall loss : {}\".format(avg_acc, avg_loss))\n",
    "        self.log.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiatlization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting GPUs. GPU list : [0]\n",
      "Main GPU : cuda:0\n",
      "CUDA is available\n",
      "Creating a directory for run dump at : /app/GraphNets/dump/gcn20191115_070248/\n",
      "Loading weights from file : /app/GraphNets/dump/gcn20191115_064647/gcn_kipf_best.pth\n",
      "Loading weights for module =  conv1\n",
      "Loading weights for module =  conv2\n",
      "Loading weights for module =  conv3\n",
      "Loading weights for module =  linear\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "engine = EngineGraph(model, config)\n",
    "# engine.train()\n",
    "engine.load_state(\"/app/GraphNets/dump/gcn20191115_064647/gcn_kipf_best.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing new validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating model on the validation set\n",
      "Iteration : 0, Progress 0.0 \n",
      "Iteration : 1, Progress 0.03571428571428571 \n",
      "Iteration : 2, Progress 0.07142857142857142 \n",
      "Iteration : 3, Progress 0.10714285714285714 \n",
      "Iteration : 4, Progress 0.14285714285714285 \n",
      "Iteration : 5, Progress 0.17857142857142858 \n",
      "Iteration : 6, Progress 0.21428571428571427 \n",
      "Iteration : 7, Progress 0.25 \n",
      "Iteration : 8, Progress 0.2857142857142857 \n",
      "Iteration : 9, Progress 0.32142857142857145 \n",
      "Iteration : 10, Progress 0.35714285714285715 \n",
      "Iteration : 11, Progress 0.39285714285714285 \n",
      "Iteration : 12, Progress 0.42857142857142855 \n",
      "Iteration : 13, Progress 0.4642857142857143 \n",
      "Iteration : 14, Progress 0.5 \n",
      "Iteration : 15, Progress 0.5357142857142857 \n",
      "Iteration : 16, Progress 0.5714285714285714 \n",
      "Iteration : 17, Progress 0.6071428571428571 \n",
      "Iteration : 18, Progress 0.6428571428571429 \n",
      "Iteration : 19, Progress 0.6785714285714286 \n",
      "Iteration : 20, Progress 0.7142857142857143 \n",
      "Iteration : 21, Progress 0.75 \n",
      "Iteration : 22, Progress 0.7857142857142857 \n",
      "Iteration : 23, Progress 0.8214285714285714 \n",
      "Iteration : 24, Progress 0.8571428571428571 \n",
      "Iteration : 25, Progress 0.8928571428571429 \n",
      "Iteration : 26, Progress 0.9285714285714286 \n",
      "Iteration : 27, Progress 0.9642857142857143 \n",
      "Overall acc : 0.33220338983050846, Overall loss : 1.101829171180725"
     ]
    }
   ],
   "source": [
    "engine.validate(\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
