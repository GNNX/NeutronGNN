{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Network\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.nn import GCNConv, ChebConv\n",
    "from torch_geometric.nn import global_max_pool\n",
    "\n",
    "# Data\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from torch_geometric.data import Batch, Data, Dataset, DataLoader\n",
    "\n",
    "# General\n",
    "import numpy as np\n",
    "\n",
    "# Util\n",
    "import os.path as osp\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate fake indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length = 2937\n",
    "# splits = 10\n",
    "# random_shuffle = np.random.permutation(length)\n",
    "# validation_indicies = random_shuffle[:length//splits]\n",
    "# test_indicies = random_shuffle[length//splits:2*length//splits]\n",
    "# train_indicies = random_shuffle[2*length//splits:]\n",
    "\n",
    "# with open(\"train_indicies.txt\", 'w') as f:\n",
    "#     f.writelines([\"{}\\n\".format(i) for i in train_indicies])\n",
    "    \n",
    "# with open(\"validation_indicies.txt\", 'w') as f:\n",
    "#     f.writelines([\"{}\\n\".format(i) for i in validation_indicies])\n",
    "\n",
    "# with open(\"test_indicies.txt\", 'w') as f:\n",
    "#     f.writelines([\"{}\\n\".format(i) for i in test_indicies])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Port from https://github.com/tkarras/progressive_growing_of_gans/blob/master/config.py\n",
    "class EasyDict(dict):\n",
    "    def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs)\n",
    "    def __getattr__(self, name): return self[name]\n",
    "    def __setattr__(self, name, value): self[name] = value\n",
    "    def __delattr__(self, name): del self[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = EasyDict()\n",
    "\n",
    "config.model_name = \"gcn_kipf\"\n",
    "\n",
    "config.data_path = \"/app/IWCDmPMT_4pi_full_tank_test.h5\"\n",
    "config.train_indices_file = \"train_indicies.txt\"\n",
    "config.val_indices_file = \"validation_indicies.txt\"\n",
    "config.test_indices_file = \"test_indicies.txt\"\n",
    "config.edge_index_pickle = \"../../visualization/edges_dict.pkl\"\n",
    "\n",
    "config.dump_path = \"dump\"\n",
    "\n",
    "config.num_data_workers = 0 # Sometime crashes if we do multiprocessing\n",
    "config.device = 'gpu'\n",
    "config.gpu_list = [0]\n",
    "\n",
    "config.batch_size = 32 # 256\n",
    "config.lr=0.01\n",
    "config.weight_decay=5e-4\n",
    "\n",
    "config.epochs = 10\n",
    "config.report_interval = 10 # 100\n",
    "config.num_val_batches  = 32\n",
    "config.valid_interval   = 100 # 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WCH5Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset storing image-like data from Water Cherenkov detector\n",
    "    memory-maps the detector data from hdf5 file\n",
    "    The detector data must be uncompresses and unchunked\n",
    "    labels are loaded into memory outright\n",
    "    No other data is currently loaded \n",
    "    \"\"\"\n",
    "\n",
    "    # Override the default implementation\n",
    "    def _download(self):\n",
    "        pass\n",
    "    \n",
    "    def _process(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def __init__(self, path, train_indices_file, val_indices_file, test_indices_file, \n",
    "                 edge_index_pickle, nodes=15808,\n",
    "                 transform=None, pre_transform=None, pre_filter=None, \n",
    "                 use_node_attr=False, use_edge_attr=False, cleaned=False):\n",
    "\n",
    "        super(WCH5Dataset, self).__init__(\"\", transform, pre_transform,\n",
    "                                        pre_filter)\n",
    "        \n",
    "        f=h5py.File(path,'r')\n",
    "        hdf5_event_data = f[\"event_data\"]\n",
    "        hdf5_labels=f[\"labels\"]\n",
    "\n",
    "        assert hdf5_event_data.shape[0] == hdf5_labels.shape[0]\n",
    "\n",
    "        event_data_shape = hdf5_event_data.shape\n",
    "        event_data_offset = hdf5_event_data.id.get_offset()\n",
    "        event_data_dtype = hdf5_event_data.dtype\n",
    "\n",
    "        #this creates a memory map - i.e. events are not loaded in memory here\n",
    "        #only on get_item\n",
    "        self.event_data = np.memmap(path, mode='r', shape=event_data_shape, \n",
    "                                    offset=event_data_offset, dtype=event_data_dtype)\n",
    "        \n",
    "        #this will fit easily in memory even for huge datasets\n",
    "        self.labels = np.array(hdf5_labels)\n",
    "        self.nodes = nodes\n",
    "        self.load_edges(edge_index_pickle)\n",
    "        \n",
    "        self.transform=transform\n",
    "        \n",
    "        #the section below handles the subset\n",
    "        #(for reduced dataset training tests)\n",
    "        #as well as shuffling and train/test/validation splits\n",
    "            \n",
    "        self.train_indices = self.load_indicies(train_indices_file)\n",
    "        self.val_indices = self.load_indicies(val_indices_file)\n",
    "        self.test_indices = self.load_indicies(test_indices_file)\n",
    "    \n",
    "    def load_indicies(self, indicies_file):\n",
    "        with open(indicies_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        # indicies = [int(l.strip()) for l in lines if not l.isspace()]\n",
    "        indicies = [int(l.strip()) for l in lines]\n",
    "        return indicies\n",
    "    \n",
    "    def load_edges(self, edge_index_pickle):\n",
    "        edge_index = torch.zeros([self.nodes, self.nodes], dtype=torch.int64)\n",
    "\n",
    "        with open(edge_index_pickle, 'rb') as f:\n",
    "            edges = pickle.load(f)\n",
    "\n",
    "            for k,vs in edges.items():\n",
    "                for v in vs:\n",
    "                    edge_index[k,v] = 1\n",
    "\n",
    "        self.edge_index=edge_index.to_sparse()._indices()\n",
    "    \n",
    "    def get(self, idx):\n",
    "        x = torch.from_numpy(self.event_data[idx])\n",
    "        y = torch.tensor([self.labels[idx]], dtype=torch.int64)\n",
    "\n",
    "        return Data(x=x, y=y, edge_index=self.edge_index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(path, train_indices_file, val_indices_file, test_indices_file, edges_dict_pickle, batch_size, workers):\n",
    "    \n",
    "    dataset = WCH5Dataset(path, train_indices_file, val_indices_file, test_indices_file, edges_dict_pickle)\n",
    "                          \n",
    "    train_loader=DataLoader(dataset, batch_size=batch_size, num_workers=workers,\n",
    "                            pin_memory=True, sampler=SubsetRandomSampler(dataset.train_indices))\n",
    "\n",
    "    val_loader=DataLoader(dataset, batch_size=batch_size, num_workers=workers,\n",
    "                            pin_memory=True, sampler=SubsetRandomSampler(dataset.val_indices))\n",
    "\n",
    "    test_loader=DataLoader(dataset, batch_size=batch_size, num_workers=workers,\n",
    "                            pin_memory=True, sampler=SubsetRandomSampler(dataset.test_indices))\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(2, 16, cached=False)\n",
    "        self.conv2 = GCNConv(16, 16, cached=False)\n",
    "        self.conv3 = GCNConv(16, 5, cached=False)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, edge_index, batch_index = batch.x, batch.edge_index, batch.batch\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = global_max_pool(x, batch_index)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python standard imports\n",
    "from abc import ABC, abstractmethod\n",
    "from time import strftime\n",
    "from os import stat, mkdir\n",
    "from math import floor, ceil\n",
    "\n",
    "# PyTorch imports\n",
    "from torch import device, load, save\n",
    "from torch.nn import DataParallel\n",
    "from torch.cuda import is_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Engine(ABC):\n",
    "\n",
    "    def __init__(self, model, config):\n",
    "        super().__init__()\n",
    "\n",
    "        # Engine attributes\n",
    "        self.model=model\n",
    "        self.config=config\n",
    "\n",
    "        # Determine the device to be used for model training and inference\n",
    "        if (config.device == 'gpu') and config.gpu_list:\n",
    "            print(\"Requesting GPUs. GPU list : \" + str(config.gpu_list))\n",
    "            self.devids=[\"cuda:{0}\".format(x) for x in config.gpu_list]\n",
    "            print(\"Main GPU : \" + self.devids[0])\n",
    "\n",
    "            if is_available():\n",
    "                self.device=device(self.devids[0])\n",
    "                if len(self.devids) > 1:\n",
    "                    print(\"Using DataParallel on these devices: {}\".format(self.devids))\n",
    "                    self.model=DataParallel(self.model, device_ids=config.gpu_list, dim=0)\n",
    "                print(\"CUDA is available\")\n",
    "            else:\n",
    "                self.device=device(\"cpu\")\n",
    "                print(\"CUDA is not available\")\n",
    "        else:\n",
    "            print(\"Unable to use GPU\")\n",
    "            self.device=device(\"cpu\")\n",
    "\n",
    "        # Send the model to the selected device\n",
    "        self.model = DataParallel(self.model) if len(self.devids) > 1 else self.model # Changed\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Setup the parameters tp save given the model type\n",
    "        if type(self.model) == DataParallel:\n",
    "            self.model_accs=self.model.module\n",
    "        else:\n",
    "            self.model_accs=self.model\n",
    "  \n",
    "        # Create the dataset object\n",
    "        out = get_loaders(config.data_path, \n",
    "                      config.train_indices_file, config.val_indices_file, config.test_indices_file,   # Changed\n",
    "                      config.edge_index_pickle, config.batch_size, config.num_data_workers)\n",
    "\n",
    "        self.train_loader, self.val_loader, self.test_loader = out\n",
    "        # Define the variant dependent attributes\n",
    "        self.criterion=None\n",
    "\n",
    "        # Create the directory for saving the log and dump files\n",
    "        self.dirpath=config.dump_path + strftime(\"%Y%m%d_%H%M%S\") + \"/\"\n",
    "        try:\n",
    "            stat(self.dirpath)\n",
    "        except:\n",
    "            print(\"Creating a directory for run dump at : {}\".format(self.dirpath))\n",
    "            mkdir(self.dirpath)\n",
    "\n",
    "        # Logging attributes\n",
    "        self.train_log=CSVData(self.dirpath + \"log_train.csv\")\n",
    "        self.val_log=CSVData(self.dirpath + \"log_val.csv\")\n",
    "\n",
    "#         # Save a copy of the config in the dump path\n",
    "#         save_config(self.config, self.dirpath + \"config_file.ini\")   # changed\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, data, mode):\n",
    "        \"\"\"Forward pass using self.data as input.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, predict, expected):\n",
    "        \"\"\"Backward pass using the loss computed for a mini-batch.\"\"\"\n",
    "        self.optimizer.zero_grad()  # Reset gradient accumulation\n",
    "        loss = self.criterion(predict, expected)\n",
    "        loss.backward()# Propagate the loss backwards\n",
    "        self.optimizer.step()       # Update the optimizer parameters         \n",
    "        \n",
    "        return loss\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self):\n",
    "        \"\"\"Training loop over the entire dataset for a given number of epochs.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save_state(self, mode=\"latest\"):\n",
    "        \"\"\"Save the model parameters in a file.\n",
    "        \n",
    "        Args :\n",
    "        mode -- one of \"latest\", \"best\" to differentiate\n",
    "                the latest model from the model with the\n",
    "                lowest loss on the validation subset (default \"latest\")\n",
    "        \"\"\"\n",
    "        path=self.dirpath + self.config.model_name + \"_\" + mode + \".pth\"\n",
    "\n",
    "        # Extract modules from the model dict and add to start_dict \n",
    "        modules=list(self.model_accs._modules.keys())\n",
    "        state_dict={module: getattr(self.model_accs, module).state_dict() for module in modules}\n",
    "\n",
    "        # Save the model parameter dict\n",
    "        save(state_dict, path)\n",
    "\n",
    "    def load_state(self, path):\n",
    "        \"\"\"Load the model parameters from a file.\n",
    "        \n",
    "        Args :\n",
    "        path -- absolute path to the .pth file containing the dictionary\n",
    "        with the model parameters to load from\n",
    "        \"\"\"\n",
    "        # Open a file in read-binary mode\n",
    "        with open(path, 'rb') as f:\n",
    "\n",
    "            # Interpret the file using torch.load()\n",
    "            checkpoint=load(f, map_location=self.device)\n",
    "\n",
    "            print(\"Loading weights from file : {0}\".format(path))\n",
    "\n",
    "            local_module_keys=list(self.model_accs._modules.keys())\n",
    "            for module in checkpoint.keys():\n",
    "                if module in local_module_keys:\n",
    "                    print(\"Loading weights for module = \", module)\n",
    "                    getattr(self.model_accs, module).load_state_dict(checkpoint[module])\n",
    "                    \n",
    "        self.model.to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python standard imports\n",
    "from sys import stdout\n",
    "from math import floor, ceil\n",
    "from time import strftime, localtime\n",
    "\n",
    "# PyTorch imports\n",
    "from torch.optim import Adam\n",
    "\n",
    "# WatChMaL imports\n",
    "# from training_utils.engine import Engine\n",
    "# from plot_utils.notebook_utils import CSVData\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVData:\n",
    "\n",
    "    def __init__(self,fout):\n",
    "        self.name  = fout\n",
    "        self._fout = None\n",
    "        self._str  = None\n",
    "        self._dict = {}\n",
    "\n",
    "    def record(self, keys, vals):\n",
    "        for i, key in enumerate(keys):\n",
    "            self._dict[key] = vals[i]\n",
    "\n",
    "    def write(self):\n",
    "        if self._str is None:\n",
    "            self._fout=open(self.name,'w')\n",
    "            self._str=''\n",
    "            for i,key in enumerate(self._dict.keys()):\n",
    "                if i:\n",
    "                    self._fout.write(',')\n",
    "                    self._str += ','\n",
    "                self._fout.write(key)\n",
    "                self._str+='{:f}'\n",
    "            self._fout.write('\\n')\n",
    "            self._str+='\\n'\n",
    "\n",
    "        self._fout.write(self._str.format(*(self._dict.values())))\n",
    "\n",
    "    def flush(self):\n",
    "        if self._fout: self._fout.flush()\n",
    "\n",
    "    def close(self):\n",
    "        if self._str is not None:\n",
    "            self._fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EngineGraph(Engine):\n",
    "\n",
    "    def __init__(self, model, config):\n",
    "        super().__init__(model, config)\n",
    "        self.criterion=F.nll_loss\n",
    "        self.optimizer=Adam(self.model_accs.parameters(), lr=config.lr)\n",
    "        \n",
    "        self.keys = ['iteration', 'epoch', 'loss', 'acc']\n",
    "\n",
    "    def forward(self, data, mode=\"train\"):\n",
    "        \"\"\"Overrides the forward abstract method in Engine.py.\n",
    "        \n",
    "        Args:\n",
    "        mode -- One of 'train', 'validation' \n",
    "        \"\"\"\n",
    "\n",
    "        # Set the correct grad_mode given the mode\n",
    "        if mode == \"train\":\n",
    "            self.model.train()\n",
    "        elif mode in [\"validation\"]:\n",
    "            self.model.eval()\n",
    "\n",
    "        return self.model(data)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Overrides the train method in Engine.py.\n",
    "        \n",
    "        Args: None\n",
    "        \"\"\"\n",
    "        \n",
    "        epochs          = self.config.epochs\n",
    "        report_interval = self.config.report_interval\n",
    "        valid_interval  = self.config.valid_interval\n",
    "        num_val_batches = self.config.num_val_batches\n",
    "\n",
    "        # Initialize counters\n",
    "        epoch=0.\n",
    "        iteration=0\n",
    "\n",
    "        # Parameter to upadte when saving the best model\n",
    "        best_loss=1000000.\n",
    "\n",
    "        val_iter = iter(self.val_loader)\n",
    "        \n",
    "        # Global training loop for multiple epochs\n",
    "        while (floor(epoch) < epochs):\n",
    "\n",
    "            print('Epoch', floor(epoch),\n",
    "                  'Starting @', strftime(\"%Y-%m-%d %H:%M:%S\", localtime()))\n",
    "\n",
    "            # Local training loop for a single epoch\n",
    "            for data in self.train_loader:\n",
    "                data = data.to(self.device)\n",
    "\n",
    "                # Update the epoch and iteration\n",
    "                epoch+=1. / len(self.train_loader)\n",
    "                iteration += 1\n",
    "                \n",
    "                # Do a forward pass using data = self.data\n",
    "                res=self.forward(data, mode=\"train\")\n",
    "\n",
    "                # Do a backward pass using loss = self.loss\n",
    "                loss = self.backward(res, data.y)\n",
    "\n",
    "                acc = res.argmax(1).eq(data.y).sum().item()/data.y.shape[0]\n",
    "                \n",
    "                # Record the metrics for the mini-batch in the log\n",
    "                self.train_log.record(self.keys, [iteration, epoch, loss, acc])\n",
    "                self.train_log.write()\n",
    "\n",
    "                # Print the metrics at given intervals\n",
    "                if iteration % report_interval == 0:\n",
    "                    print(\"... Iteration %d ... Epoch %1.2f ... Loss %1.3f ... Acc %1.3f\"\n",
    "                          % (iteration, epoch, loss, acc))\n",
    "\n",
    "                # Run validation on given intervals\n",
    "                if iteration % valid_interval == 0:\n",
    "                    with torch.no_grad():\n",
    "                        val_loss=0.\n",
    "                        val_acc=0.\n",
    "\n",
    "                        for val_batch in range(num_val_batches):\n",
    "\n",
    "                            try:\n",
    "                                data=next(val_iter)\n",
    "                            except StopIteration:\n",
    "                                val_iter=iter(self.val_loader)\n",
    "                                data=next(val_iter)\n",
    "                            data = data.to(self.device)\n",
    "\n",
    "                            # Extract the event data from the input data tuple\n",
    "                            res=self.forward(data, mode=\"validation\")\n",
    "                            acc = res.argmax(1).eq(data.y).sum().item()/data.y.shape[0]\n",
    "\n",
    "                            val_loss+=self.criterion(res, data.y)\n",
    "                            val_acc+=acc\n",
    "\n",
    "                        val_loss /= num_val_batches\n",
    "                        val_acc /= num_val_batches\n",
    "\n",
    "                        # Record the validation stats to the csv\n",
    "                        self.val_log.record(self.keys, [iteration, epoch, loss, acc])\n",
    "                        self.val_log.write()\n",
    "\n",
    "                        # Save the best model\n",
    "                        if val_loss < best_loss:\n",
    "                            self.save_state(mode=\"best\")\n",
    "                            best_loss = val_loss\n",
    "\n",
    "                        # Save the latest model\n",
    "                        self.save_state(mode=\"latest\")\n",
    "                    \n",
    "\n",
    "        self.val_log.close()\n",
    "        self.train_log.close()\n",
    "\n",
    "    def validate(self, subset):\n",
    "        \"\"\"Overrides the validate method in Engine.py.\n",
    "        \n",
    "        Args:\n",
    "        subset          -- One of 'train', 'validation', 'test' to select the subset to perform validation on\n",
    "        \"\"\"\n",
    "        # Print start message\n",
    "        if subset == \"train\":\n",
    "            message=\"Validating model on the train set\"\n",
    "        elif subset == \"validation\":\n",
    "            message=\"Validating model on the validation set\"\n",
    "        elif subset == \"test\":\n",
    "            message=\"Validating model on the test set\"\n",
    "        else:\n",
    "            print(\"validate() : arg subset has to be one of train, validation, test\")\n",
    "            return None\n",
    "\n",
    "        print(message)\n",
    "        \n",
    "        # Setup the CSV file for logging the output, path to save the actual and reconstructed events, dataloader iterator\n",
    "        if subset == \"train\":\n",
    "            self.log=CSVData(self.dirpath + \"train_validation_log.csv\")\n",
    "            np_event_path=self.dirpath + \"/train_valid_iteration_\"\n",
    "            data_iter=self.train_loader\n",
    "        elif subset == \"validation\":\n",
    "            self.log=CSVData(self.dirpath + \"valid_validation_log.csv\")\n",
    "            np_event_path=self.dirpath + \"/val_valid_iteration_\"\n",
    "            data_iter=self.val_loader\n",
    "        else:\n",
    "            self.log=CSVData(self.dirpath + \"test_validation_log.csv\")\n",
    "            np_event_path=self.dirpath + \"/test_validation_iteration_\"\n",
    "            data_iter=self.test_loader\n",
    "\n",
    "        save_arr_dict={\"events\": [], \"labels\": [], \"energies\": []}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for iteration, data in enumerate(data_iter):\n",
    "                data = data.to(self.device)\n",
    "\n",
    "                stdout.write(\"Iteration : {}, Progress {} \\n\".format(iteration, iteration/len(data_iter)))\n",
    "                res=self.forward(data, mode=\"validation\")\n",
    "                acc = res.argmax(1).eq(data.y).sum().item()/data.y.shape[0]\n",
    "                loss = self.criterion(res, data.y)\n",
    "\n",
    "                # Log/Report\n",
    "                self.log.record([\"Iteration\", \"loss\", \"acc\"], [iteration, loss, acc])\n",
    "                self.log.write()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiatlization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting GPUs. GPU list : [0]\n",
      "Main GPU : cuda:0\n",
      "CUDA is available\n",
      "Creating a directory for run dump at : dump20191104_233104/\n"
     ]
    }
   ],
   "source": [
    "engine = EngineGraph(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Starting @ 2019-11-04 23:31:04\n",
      "... Iteration 10 ... Epoch 0.14 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 20 ... Epoch 0.27 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 30 ... Epoch 0.41 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 40 ... Epoch 0.54 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 50 ... Epoch 0.68 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 60 ... Epoch 0.81 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 70 ... Epoch 0.95 ... Loss 0.000 ... Acc 1.000\n",
      "Epoch 0 Starting @ 2019-11-04 23:31:12\n",
      "... Iteration 80 ... Epoch 1.08 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 90 ... Epoch 1.22 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 100 ... Epoch 1.35 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 110 ... Epoch 1.49 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 120 ... Epoch 1.62 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 130 ... Epoch 1.76 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 140 ... Epoch 1.89 ... Loss 0.000 ... Acc 1.000\n",
      "Epoch 2 Starting @ 2019-11-04 23:31:21\n",
      "... Iteration 150 ... Epoch 2.03 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 160 ... Epoch 2.16 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 170 ... Epoch 2.30 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 180 ... Epoch 2.43 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 190 ... Epoch 2.57 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 200 ... Epoch 2.70 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 210 ... Epoch 2.84 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 220 ... Epoch 2.97 ... Loss 0.000 ... Acc 1.000\n",
      "Epoch 3 Starting @ 2019-11-04 23:31:30\n",
      "... Iteration 230 ... Epoch 3.11 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 240 ... Epoch 3.24 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 250 ... Epoch 3.38 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 260 ... Epoch 3.51 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 270 ... Epoch 3.65 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 280 ... Epoch 3.78 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 290 ... Epoch 3.92 ... Loss 0.000 ... Acc 1.000\n",
      "Epoch 4 Starting @ 2019-11-04 23:31:37\n",
      "... Iteration 300 ... Epoch 4.05 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 310 ... Epoch 4.19 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 320 ... Epoch 4.32 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 330 ... Epoch 4.46 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 340 ... Epoch 4.59 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 350 ... Epoch 4.73 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 360 ... Epoch 4.86 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 370 ... Epoch 5.00 ... Loss 0.000 ... Acc 1.000\n",
      "Epoch 4 Starting @ 2019-11-04 23:31:47\n",
      "... Iteration 380 ... Epoch 5.14 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 390 ... Epoch 5.27 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 400 ... Epoch 5.41 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 410 ... Epoch 5.54 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 420 ... Epoch 5.68 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 430 ... Epoch 5.81 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 440 ... Epoch 5.95 ... Loss 0.000 ... Acc 1.000\n",
      "Epoch 5 Starting @ 2019-11-04 23:31:56\n",
      "... Iteration 450 ... Epoch 6.08 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 460 ... Epoch 6.22 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 470 ... Epoch 6.35 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 480 ... Epoch 6.49 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 490 ... Epoch 6.62 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 500 ... Epoch 6.76 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 510 ... Epoch 6.89 ... Loss 0.000 ... Acc 1.000\n",
      "Epoch 6 Starting @ 2019-11-04 23:32:05\n",
      "... Iteration 520 ... Epoch 7.03 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 530 ... Epoch 7.16 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 540 ... Epoch 7.30 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 550 ... Epoch 7.43 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 560 ... Epoch 7.57 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 570 ... Epoch 7.70 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 580 ... Epoch 7.84 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 590 ... Epoch 7.97 ... Loss 0.000 ... Acc 1.000\n",
      "Epoch 7 Starting @ 2019-11-04 23:32:12\n",
      "... Iteration 600 ... Epoch 8.11 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 610 ... Epoch 8.24 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 620 ... Epoch 8.38 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 630 ... Epoch 8.51 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 640 ... Epoch 8.65 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 650 ... Epoch 8.78 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 660 ... Epoch 8.92 ... Loss 0.000 ... Acc 1.000\n",
      "Epoch 8 Starting @ 2019-11-04 23:32:22\n",
      "... Iteration 670 ... Epoch 9.05 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 680 ... Epoch 9.19 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 690 ... Epoch 9.32 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 700 ... Epoch 9.46 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 710 ... Epoch 9.59 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 720 ... Epoch 9.73 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 730 ... Epoch 9.86 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 740 ... Epoch 10.00 ... Loss 0.000 ... Acc 1.000\n",
      "Epoch 9 Starting @ 2019-11-04 23:32:31\n",
      "... Iteration 750 ... Epoch 10.14 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 760 ... Epoch 10.27 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 770 ... Epoch 10.41 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 780 ... Epoch 10.54 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 790 ... Epoch 10.68 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 800 ... Epoch 10.81 ... Loss 0.000 ... Acc 1.000\n",
      "... Iteration 810 ... Epoch 10.95 ... Loss 0.000 ... Acc 1.000\n"
     ]
    }
   ],
   "source": [
    "engine.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating model on the test set\n",
      "Iteration : 0, Progress 0.0 \n",
      "Iteration : 1, Progress 0.1 \n",
      "Iteration : 2, Progress 0.2 \n",
      "Iteration : 3, Progress 0.3 \n",
      "Iteration : 4, Progress 0.4 \n",
      "Iteration : 5, Progress 0.5 \n",
      "Iteration : 6, Progress 0.6 \n",
      "Iteration : 7, Progress 0.7 \n",
      "Iteration : 8, Progress 0.8 \n",
      "Iteration : 9, Progress 0.9 \n"
     ]
    }
   ],
   "source": [
    "engine.validate(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.3599,  0.0140, -0.3149,  0.2350, -0.5300,  0.2933, -0.0255, -0.3360,\n",
       "         -0.3423, -0.5331, -0.5557,  0.5384, -0.4086, -0.3214,  0.1107,  0.0660],\n",
       "        [-0.3054,  0.2230,  0.2069, -0.0185,  0.0365,  0.3244,  0.6505,  0.2952,\n",
       "         -0.4026, -0.4148,  0.4713,  0.5834, -0.2416,  0.4654,  0.1707,  0.3192]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.5091, 0.4904, 0.5061, 0.6607, 0.9020, 0.1005, 0.7182, 0.2354, 0.0719,\n",
       "         0.8242, 0.8879, 0.4866, 0.2581, 0.4314, 0.2023, 0.3790],\n",
       "        [0.4410, 0.9453, 0.6468, 0.1219, 0.9758, 0.5727, 0.4021, 0.8795, 0.8744,\n",
       "         0.3221, 0.5032, 0.0516, 0.3337, 0.1342, 0.2977, 0.0199]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv1.weight = torch.nn.Parameter(torch.Tensor(np.random.rand(2,16)))\n",
    "model.conv1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from file : dump20191104_233104/gcn_kipf_latest.pth\n",
      "Loading weights for module =  conv1\n",
      "Loading weights for module =  conv2\n",
      "Loading weights for module =  conv3\n"
     ]
    }
   ],
   "source": [
    "engine.load_state(osp.join(engine.dirpath, \"gcn_kipf_latest.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.3599,  0.0140, -0.3149,  0.2350, -0.5300,  0.2933, -0.0255, -0.3360,\n",
       "         -0.3423, -0.5331, -0.5557,  0.5384, -0.4086, -0.3214,  0.1107,  0.0660],\n",
       "        [-0.3054,  0.2230,  0.2069, -0.0185,  0.0365,  0.3244,  0.6505,  0.2952,\n",
       "         -0.4026, -0.4148,  0.4713,  0.5834, -0.2416,  0.4654,  0.1707,  0.3192]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv1.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model works after loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating model on the test set\n",
      "Iteration : 0, Progress 0.0 \n",
      "Iteration : 1, Progress 0.1 \n",
      "Iteration : 2, Progress 0.2 \n",
      "Iteration : 3, Progress 0.3 \n",
      "Iteration : 4, Progress 0.4 \n",
      "Iteration : 5, Progress 0.5 \n",
      "Iteration : 6, Progress 0.6 \n",
      "Iteration : 7, Progress 0.7 \n",
      "Iteration : 8, Progress 0.8 \n",
      "Iteration : 9, Progress 0.9 \n"
     ]
    }
   ],
   "source": [
    "engine.validate(\"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
