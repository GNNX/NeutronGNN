{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Network\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.nn import GCNConv, ChebConv\n",
    "from torch_geometric.nn import global_max_pool\n",
    "\n",
    "# Data\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, Sampler\n",
    "\n",
    "from torch_geometric.data import Batch, Data, Dataset, DataLoader\n",
    "\n",
    "# General\n",
    "import numpy as np\n",
    "\n",
    "# Util\n",
    "import os\n",
    "import os.path as osp\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate fake indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length = 2937\n",
    "# splits = 10\n",
    "# random_shuffle = np.random.permutation(length)\n",
    "# validation_indicies = random_shuffle[:length//splits]\n",
    "# test_indicies = random_shuffle[length//splits:2*length//splits]\n",
    "# train_indicies = random_shuffle[2*length//splits:]\n",
    "\n",
    "# with open(\"train_indicies.txt\", 'w') as f:\n",
    "#     f.writelines([\"{}\\n\".format(i) for i in train_indicies])\n",
    "    \n",
    "# with open(\"validation_indicies.txt\", 'w') as f:\n",
    "#     f.writelines([\"{}\\n\".format(i) for i in validation_indicies])\n",
    "\n",
    "# with open(\"test_indicies.txt\", 'w') as f:\n",
    "#     f.writelines([\"{}\\n\".format(i) for i in test_indicies])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Port from https://github.com/tkarras/progressive_growing_of_gans/blob/master/config.py\n",
    "class EasyDict(dict):\n",
    "    def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs)\n",
    "    def __getattr__(self, name): return self[name]\n",
    "    def __setattr__(self, name, value): self[name] = value\n",
    "    def __delattr__(self, name): del self[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = EasyDict()\n",
    "\n",
    "config.model_name = \"gcn_kipf\"\n",
    "\n",
    "## Data paths\n",
    "config.data_path = \"/app/test_data/split_h5/IWCDmPMT_4pi_fulltank_test_graphnet_trainval.h5\"\n",
    "config.train_indices_file = \"/app/test_data/split_h5/IWCDmPMT_4pi_fulltank_test_graphnet_splits/train.txt\"\n",
    "config.val_indices_file = \"/app/test_data/split_h5/IWCDmPMT_4pi_fulltank_test_graphnet_splits/val.txt\"\n",
    "config.edge_index_pickle = \"/app/GraphNets/metadata/edges_dict.pkl\"\n",
    "\n",
    "## Log location\n",
    "config.dump_path = \"/app/GraphNets/dump/gcn\"\n",
    "\n",
    "## Computer Parameters\n",
    "config.num_data_workers = 0 # Sometime crashes if we do multiprocessing\n",
    "config.device = 'gpu'\n",
    "config.gpu_list = [0]\n",
    "\n",
    "## Training parameters\n",
    "config.batch_size = 32 \n",
    "config.lr=0.01\n",
    "config.weight_decay=5e-4\n",
    "config.epochs = 1\n",
    "\n",
    "## Logging parameters for training\n",
    "config.report_interval = 10 # 100\n",
    "config.num_val_batches  = 32\n",
    "config.valid_interval   = 100 # 10000\n",
    "\n",
    "## Validating parameters\n",
    "config.validate_batch_size = 32\n",
    "config.validate_dump_interval = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WCH5Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset storing image-like data from Water Cherenkov detector\n",
    "    memory-maps the detector data from hdf5 file\n",
    "    The detector data must be uncompresses and unchunked\n",
    "    labels are loaded into memory outright\n",
    "    No other data is currently loaded \n",
    "    \"\"\"\n",
    "\n",
    "    # Override the default implementation\n",
    "    def _download(self):\n",
    "        pass\n",
    "    \n",
    "    def _process(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def __init__(self, path, edge_index_pickle, nodes=15808,\n",
    "                 transform=None, pre_transform=None, pre_filter=None, \n",
    "                 use_node_attr=False, use_edge_attr=False, cleaned=False):\n",
    "\n",
    "        super(WCH5Dataset, self).__init__(\"\", transform, pre_transform,\n",
    "                                        pre_filter)\n",
    "        \n",
    "        f=h5py.File(path,'r')\n",
    "        hdf5_event_data = f[\"event_data\"]\n",
    "        hdf5_labels=f[\"labels\"]\n",
    "\n",
    "        assert hdf5_event_data.shape[0] == hdf5_labels.shape[0]\n",
    "\n",
    "        event_data_shape = hdf5_event_data.shape\n",
    "        event_data_offset = hdf5_event_data.id.get_offset()\n",
    "        event_data_dtype = hdf5_event_data.dtype\n",
    "\n",
    "        #this creates a memory map - i.e. events are not loaded in memory here\n",
    "        #only on get_item\n",
    "        self.event_data = np.memmap(path, mode='r', shape=event_data_shape, \n",
    "                                    offset=event_data_offset, dtype=event_data_dtype)\n",
    "        \n",
    "        #this will fit easily in memory even for huge datasets\n",
    "        self.labels = np.array(hdf5_labels)\n",
    "        self.nodes = nodes\n",
    "        self.load_edges(edge_index_pickle)\n",
    "        \n",
    "        self.transform=transform\n",
    "    \n",
    "    def load_edges(self, edge_index_pickle):\n",
    "        edge_index = torch.zeros([self.nodes, self.nodes], dtype=torch.int64)\n",
    "\n",
    "        with open(edge_index_pickle, 'rb') as f:\n",
    "            edges = pickle.load(f)\n",
    "\n",
    "            for k,vs in edges.items():\n",
    "                for v in vs:\n",
    "                    edge_index[k,v] = 1\n",
    "\n",
    "        self.edge_index=edge_index.to_sparse()._indices()\n",
    "    \n",
    "    def get(self, idx):\n",
    "        x = torch.from_numpy(self.event_data[idx])\n",
    "        y = torch.tensor([self.labels[idx]], dtype=torch.int64)\n",
    "\n",
    "        return Data(x=x, y=y, edge_index=self.edge_index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_indicies(indicies_file):\n",
    "    with open(indicies_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    # indicies = [int(l.strip()) for l in lines if not l.isspace()]\n",
    "    indicies = [int(l.strip()) for l in lines]\n",
    "    return indicies\n",
    "\n",
    "\n",
    "class WCH5Dataset_trainval(WCH5Dataset):\n",
    "    \"\"\"\n",
    "    Dataset storing image-like data from Water Cherenkov detector\n",
    "    memory-maps the detector data from hdf5 file\n",
    "    The detector data must be uncompresses and unchunked\n",
    "    labels are loaded into memory outright\n",
    "    No other data is currently loaded \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, path, train_indices_file, val_indices_file, \n",
    "                 edge_index_pickle, nodes=15808,\n",
    "                 transform=None, pre_transform=None, pre_filter=None, \n",
    "                 use_node_attr=False, use_edge_attr=False, cleaned=False):\n",
    "\n",
    "        super(WCH5Dataset_trainval, self).__init__( path, \n",
    "                 edge_index_pickle, nodes=nodes,\n",
    "                 transform=transform, pre_transform=pre_transform, pre_filter=pre_filter, \n",
    "                 use_node_attr=use_node_attr, use_edge_attr=use_edge_attr, cleaned=cleaned)\n",
    "        \n",
    "            \n",
    "        self.train_indices = load_indicies(train_indices_file)\n",
    "        self.val_indices = load_indicies(val_indices_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(path, train_indices_file, val_indices_file, edges_dict_pickle, batch_size, workers):\n",
    "    \n",
    "    dataset = WCH5Dataset_trainval(path, train_indices_file, val_indices_file, edges_dict_pickle)\n",
    "                          \n",
    "    train_loader=DataLoader(dataset, batch_size=batch_size, num_workers=workers,\n",
    "                            pin_memory=True, sampler=SubsetRandomSampler(dataset.train_indices))\n",
    "\n",
    "    val_loader=DataLoader(dataset, batch_size=batch_size, num_workers=workers,\n",
    "                            pin_memory=True, sampler=SubsetRandomSampler(dataset.val_indices))\n",
    "\n",
    "    return train_loader, val_loader, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubsetSequentialSampler(Sampler):\n",
    "    r\"\"\"Samples elements randomly from a given list of indices, without replacement.\n",
    "\n",
    "    Arguments:\n",
    "        indices (sequence): a sequence of indices\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, indices):\n",
    "        self.indices = indices\n",
    "\n",
    "    def __iter__(self):\n",
    "#         return (i for i in self.indices)\n",
    "        return iter(self.indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(2, 4, cached=False)\n",
    "        self.conv2 = GCNConv(4, 4, cached=False)\n",
    "        self.conv3 = GCNConv(4, 4, cached=False)\n",
    "        self.linear = Linear(4, 3)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, edge_index, batch_index = batch.x, batch.edge_index, batch.batch\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = global_max_pool(x, batch_index)\n",
    "        x = self.linear(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python standard imports\n",
    "from abc import ABC, abstractmethod\n",
    "from time import strftime\n",
    "from os import stat, mkdir\n",
    "from math import floor, ceil\n",
    "\n",
    "# PyTorch imports\n",
    "from torch import device, load, save\n",
    "from torch.nn import DataParallel\n",
    "from torch.cuda import is_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Engine(ABC):\n",
    "\n",
    "    def __init__(self, model, config):\n",
    "        super().__init__()\n",
    "\n",
    "        # Engine attributes\n",
    "        self.model=model\n",
    "        self.config=config\n",
    "\n",
    "        # Determine the device to be used for model training and inference\n",
    "        if (config.device == 'gpu') and config.gpu_list:\n",
    "            print(\"Requesting GPUs. GPU list : \" + str(config.gpu_list))\n",
    "            self.devids=[\"cuda:{0}\".format(x) for x in config.gpu_list]\n",
    "            print(\"Main GPU : \" + self.devids[0])\n",
    "\n",
    "            if is_available():\n",
    "                self.device=device(self.devids[0])\n",
    "                if len(self.devids) > 1:\n",
    "                    print(\"Using DataParallel on these devices: {}\".format(self.devids))\n",
    "                    self.model=DataParallel(self.model, device_ids=config.gpu_list, dim=0)\n",
    "                print(\"CUDA is available\")\n",
    "            else:\n",
    "                self.device=device(\"cpu\")\n",
    "                print(\"CUDA is not available\")\n",
    "        else:\n",
    "            print(\"Unable to use GPU\")\n",
    "            self.device=device(\"cpu\")\n",
    "\n",
    "        # Send the model to the selected device\n",
    "        self.model = DataParallel(self.model) if len(self.devids) > 1 else self.model # Changed\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Setup the parameters tp save given the model type\n",
    "        if type(self.model) == DataParallel:\n",
    "            self.model_accs=self.model.module\n",
    "        else:\n",
    "            self.model_accs=self.model\n",
    "  \n",
    "        # Create the dataset object\n",
    "        out = get_loaders(config.data_path, \n",
    "                      config.train_indices_file, config.val_indices_file,   \n",
    "                      config.edge_index_pickle, config.batch_size, config.num_data_workers)\n",
    "\n",
    "        self.train_loader, self.val_loader, self.dataset = out\n",
    "        # Define the variant dependent attributes\n",
    "        self.criterion=None\n",
    "\n",
    "        # Create the directory for saving the log and dump files\n",
    "        self.dirpath=config.dump_path + strftime(\"%Y%m%d_%H%M%S\") + \"/\"\n",
    "        try:\n",
    "            stat(self.dirpath)\n",
    "        except:\n",
    "            print(\"Creating a directory for run dump at : {}\".format(self.dirpath))\n",
    "            mkdir(self.dirpath)\n",
    "\n",
    "        # Logging attributes\n",
    "        self.train_log=CSVData(self.dirpath + \"log_train.csv\")\n",
    "        self.val_log=CSVData(self.dirpath + \"log_val.csv\")\n",
    "\n",
    "#         # Save a copy of the config in the dump path\n",
    "#         save_config(self.config, self.dirpath + \"config_file.ini\")   # changed\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, data, mode):\n",
    "        \"\"\"Forward pass using self.data as input.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, predict, expected):\n",
    "        \"\"\"Backward pass using the loss computed for a mini-batch.\"\"\"\n",
    "        self.optimizer.zero_grad()  # Reset gradient accumulation\n",
    "        loss = self.criterion(predict, expected)\n",
    "        loss.backward()# Propagate the loss backwards\n",
    "        self.optimizer.step()       # Update the optimizer parameters         \n",
    "        \n",
    "        return loss\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self):\n",
    "        \"\"\"Training loop over the entire dataset for a given number of epochs.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save_state(self, mode=\"latest\", name=\"\"):\n",
    "        \"\"\"Save the model parameters in a file.\n",
    "        \n",
    "        Args :\n",
    "        mode -- one of \"latest\", \"best\" to differentiate\n",
    "                the latest model from the model with the\n",
    "                lowest loss on the validation subset (default \"latest\")\n",
    "        \"\"\"\n",
    "        if name:\n",
    "            path=self.dirpath + self.config.model_name + \"_\" + mode + \"_\" + name + \".pth\"\n",
    "        else:\n",
    "            path=self.dirpath + self.config.model_name + \"_\" + mode + \".pth\"\n",
    "\n",
    "        # Extract modules from the model dict and add to start_dict \n",
    "        modules=list(self.model_accs._modules.keys())\n",
    "        state_dict={module: getattr(self.model_accs, module).state_dict() for module in modules}\n",
    "\n",
    "        # Save the model parameter dict\n",
    "        save(state_dict, path)\n",
    "\n",
    "    def load_state(self, path):\n",
    "        \"\"\"Load the model parameters from a file.\n",
    "        \n",
    "        Args :\n",
    "        path -- absolute path to the .pth file containing the dictionary\n",
    "        with the model parameters to load from\n",
    "        \"\"\"\n",
    "        # Open a file in read-binary mode\n",
    "        with open(path, 'rb') as f:\n",
    "\n",
    "            # Interpret the file using torch.load()\n",
    "            checkpoint=load(f, map_location=self.device)\n",
    "\n",
    "            print(\"Loading weights from file : {0}\".format(path))\n",
    "\n",
    "            local_module_keys=list(self.model_accs._modules.keys())\n",
    "            for module in checkpoint.keys():\n",
    "                if module in local_module_keys:\n",
    "                    print(\"Loading weights for module = \", module)\n",
    "                    getattr(self.model_accs, module).load_state_dict(checkpoint[module])\n",
    "                    \n",
    "        self.model.to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python standard imports\n",
    "from sys import stdout\n",
    "from math import floor, ceil\n",
    "from time import strftime, localtime\n",
    "\n",
    "# PyTorch imports\n",
    "from torch.optim import Adam\n",
    "\n",
    "# WatChMaL imports\n",
    "# from training_utils.engine import Engine\n",
    "# from plot_utils.notebook_utils import CSVData\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVData:\n",
    "\n",
    "    def __init__(self,fout):\n",
    "        self.name  = fout\n",
    "        self._fout = None\n",
    "        self._str  = None\n",
    "        self._dict = {}\n",
    "\n",
    "    def record(self, keys, vals):\n",
    "        for i, key in enumerate(keys):\n",
    "            self._dict[key] = vals[i]\n",
    "\n",
    "    def write(self):\n",
    "        if self._str is None:\n",
    "            self._fout=open(self.name,'w')\n",
    "            self._str=''\n",
    "            for i,key in enumerate(self._dict.keys()):\n",
    "                if i:\n",
    "                    self._fout.write(',')\n",
    "                    self._str += ','\n",
    "                self._fout.write(key)\n",
    "                self._str+='{:f}'\n",
    "            self._fout.write('\\n')\n",
    "            self._str+='\\n'\n",
    "\n",
    "        self._fout.write(self._str.format(*(self._dict.values())))\n",
    "        self.flush()\n",
    "\n",
    "    def flush(self):\n",
    "        if self._fout: self._fout.flush()\n",
    "\n",
    "    def close(self):\n",
    "        if self._str is not None:\n",
    "            self._fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EngineGraph(Engine):\n",
    "\n",
    "    def __init__(self, model, config):\n",
    "        super().__init__(model, config)\n",
    "        self.criterion=F.nll_loss\n",
    "        self.optimizer=Adam(self.model_accs.parameters(), lr=config.lr)\n",
    "        \n",
    "        self.keys = ['iteration', 'epoch', 'loss', 'acc']\n",
    "\n",
    "    def forward(self, data, mode=\"train\"):\n",
    "        \"\"\"Overrides the forward abstract method in Engine.py.\n",
    "        \n",
    "        Args:\n",
    "        mode -- One of 'train', 'validation' \n",
    "        \"\"\"\n",
    "\n",
    "        # Set the correct grad_mode given the mode\n",
    "        if mode == \"train\":\n",
    "            self.model.train()\n",
    "        elif mode in [\"validation\"]:\n",
    "            self.model.eval()\n",
    "\n",
    "        return self.model(data)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Overrides the train method in Engine.py.\n",
    "        \n",
    "        Args: None\n",
    "        \"\"\"\n",
    "        \n",
    "        epochs          = self.config.epochs\n",
    "        report_interval = self.config.report_interval\n",
    "        valid_interval  = self.config.valid_interval\n",
    "        num_val_batches = self.config.num_val_batches\n",
    "\n",
    "        # Initialize counters\n",
    "        epoch=0.\n",
    "        iteration=0\n",
    "\n",
    "        # Parameter to upadte when saving the best model\n",
    "        best_val_loss=1000000.\n",
    "        avg_val_loss=1000.\n",
    "        \n",
    "        val_iter = iter(self.val_loader)\n",
    "        \n",
    "        # Global training loop for multiple epochs\n",
    "        while (floor(epoch) < epochs):\n",
    "\n",
    "            print('Epoch', np.round(epoch).astype(np.int),\n",
    "                  'Starting @', strftime(\"%Y-%m-%d %H:%M:%S\", localtime()))\n",
    "\n",
    "            # Local training loop for a single epoch\n",
    "            for data in self.train_loader:\n",
    "                data = data.to(self.device)\n",
    "\n",
    "                # Update the epoch and iteration\n",
    "                epoch+=1. / len(self.train_loader)\n",
    "                iteration += 1\n",
    "                \n",
    "                # Do a forward pass \n",
    "                res=self.forward(data, mode=\"train\")\n",
    "\n",
    "                # Do a backward pass \n",
    "                loss = self.backward(res, data.y)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                acc = res.argmax(1).eq(data.y).sum().item()/data.y.shape[0]\n",
    "                \n",
    "                # Record the metrics for the mini-batch in the log\n",
    "                self.train_log.record(self.keys, [iteration, epoch, loss, acc])\n",
    "                self.train_log.write()\n",
    "\n",
    "                # Print the metrics at report_intervals\n",
    "                if iteration % report_interval == 0:\n",
    "                    print(\"... Iteration %d ... Epoch %1.2f ... Loss %1.3f ... Acc %1.3f\"\n",
    "                          % (iteration, epoch, loss, acc))\n",
    "\n",
    "                # Run validation on valid_intervals\n",
    "                if iteration % valid_interval == 0:\n",
    "                    val_loss=0.\n",
    "                    val_acc=0.\n",
    "                    with torch.no_grad():\n",
    "                        for val_batch in range(num_val_batches):\n",
    "                            try:\n",
    "                                data=next(val_iter)\n",
    "                            except StopIteration:\n",
    "                                val_iter=iter(self.val_loader)\n",
    "                                data=next(val_iter)\n",
    "                            data = data.to(self.device)\n",
    "\n",
    "                            # Extract the event data from the input data tuple\n",
    "                            res=self.forward(data, mode=\"validation\")\n",
    "                            acc = res.argmax(1).eq(data.y).sum().item()/data.y.shape[0]\n",
    "\n",
    "                            val_loss+=self.criterion(res, data.y)\n",
    "                            val_acc+=acc\n",
    "\n",
    "                    val_loss /= num_val_batches\n",
    "                    val_acc /= num_val_batches\n",
    "\n",
    "                    # Record the validation stats to the csv\n",
    "                    self.val_log.record(self.keys, [iteration, epoch, loss, acc])\n",
    "                    self.val_log.write()\n",
    "\n",
    "                    # Save the best model\n",
    "                    if val_loss < avg_val_loss:\n",
    "                        self.save_state(mode=\"best\", name=\"{}_{}\".format(iteration, val_loss))\n",
    "                        best_val_loss = val_loss\n",
    "                        avg_val_loss = (val_loss * avg_val_loss)**0.5\n",
    "\n",
    "                    # Save the latest model\n",
    "                    self.save_state(mode=\"latest\")\n",
    "                    \n",
    "            self.save_state(mode=\"latest\", name=\"epoch_{}\".format(np.round(epoch).astype(np.int)))\n",
    "\n",
    "        self.val_log.close()\n",
    "        self.train_log.close()\n",
    "\n",
    "    def validate(self, subset, name=\"current\"):\n",
    "        \"\"\"Overrides the validate method in Engine.py.\n",
    "        \n",
    "        Args:\n",
    "        subset          -- One of 'train', 'validation', 'test' to select the subset to perform validation on\n",
    "        \"\"\"\n",
    "        # Print start message\n",
    "        if subset == \"train\":\n",
    "            message=\"Validating model on the train set\"\n",
    "        elif subset == \"validation\":\n",
    "            message=\"Validating model on the validation set\"\n",
    "        else:\n",
    "            print(\"validate() : arg subset has to be one of train, validation, test\")\n",
    "            return None\n",
    "\n",
    "        print(message)\n",
    "        \n",
    "        # Setup the path to save output\n",
    "        # Setup indices to use\n",
    "        if subset == \"train\":\n",
    "            self.log=CSVData(self.dirpath + \"train_validation_log_{}.csv\".format(name))\n",
    "            output_path=os.path.join(self.dirpath, \"train_validation_{}\".format(name))\n",
    "            validate_indices = self.dataset.train_indices\n",
    "        else:\n",
    "            self.log=CSVData(self.dirpath + \"valid_validation_log_{}.csv\".format(name))\n",
    "            output_path=os.path.join(self.dirpath, \"valid_validation_{}\".format(name))\n",
    "            validate_indices = self.dataset.val_indices\n",
    "\n",
    "        os.makedirs(output_path)\n",
    "        data_iter = DataLoader(self.dataset, batch_size=self.config.validate_batch_size, \n",
    "                               num_workers=self.config.num_data_workers,\n",
    "                               pin_memory=True, sampler=SubsetSequentialSampler(validate_indices))\n",
    "\n",
    "        key_val = {\"index\":[], \"label\":[], \"pred\":[], \"pred_val\":[]}\n",
    "\n",
    "        avg_loss = 0\n",
    "        avg_acc = 0\n",
    "        indices_iter = iter(validate_indices)\n",
    "        \n",
    "        dump_interval = config.validate_dump_interval\n",
    "        dump_index = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for iteration, data in enumerate(data_iter):\n",
    "                gpu_data = data.to(self.device)\n",
    "\n",
    "                stdout.write(\"Iteration : {}, Progress {} \\n\".format(iteration, iteration/len(data_iter)))\n",
    "                res=self.forward(gpu_data, mode=\"validation\")\n",
    "\n",
    "                acc = res.argmax(1).eq(gpu_data.y).sum().item()\n",
    "                loss = self.criterion(res, gpu_data.y) * data.y.shape[0]\n",
    "                avg_acc += acc\n",
    "                avg_loss += loss\n",
    "\n",
    "                # Log/Report\n",
    "                self.log.record([\"Iteration\", \"loss\", \"acc\"], [iteration, loss, acc/data.y.shape[0]])\n",
    "                self.log.write()\n",
    "                \n",
    "                # Log/Report\n",
    "                for label, pred, preds in zip(data.y.tolist(), res.argmax(1).tolist(), res.exp().tolist()):\n",
    "                    key_val[\"index\"].append(next(indices_iter))\n",
    "                    key_val[\"label\"].append(label)\n",
    "                    key_val[\"pred\"].append(pred)\n",
    "                    key_val[\"pred_val\"].append(preds)\n",
    "                        \n",
    "                # Check if iteration is valid_dump_interval\n",
    "                if len(key_val[\"index\"]) >= dump_interval:\n",
    "                    print(\"dumping\")\n",
    "                    name = os.path.join(output_path, \"{}.npz\".format(dump_index))\n",
    "                    np.savez(name, **key_val)\n",
    "                    \n",
    "                    dump_index += 1\n",
    "                    key_val = {key:[] for key in key_val}\n",
    "\n",
    "        self.log.close()\n",
    "        \n",
    "        name = os.path.join(output_path, \"{}.npz\".format(dump_index))\n",
    "        np.savez(name, **key_val)\n",
    "        \n",
    "        avg_acc/=len(validate_indices)\n",
    "        avg_loss/=len(validate_indices)\n",
    "\n",
    "        stdout.write(\"Overall acc : {}, Overall loss : {}\".format(avg_acc, avg_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiatlization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting GPUs. GPU list : [0]\n",
      "Main GPU : cuda:0\n",
      "CUDA is available\n",
      "Creating a directory for run dump at : /app/GraphNets/dump/gcn20191119_060822/\n",
      "Epoch 0 Starting @ 2019-11-19 06:08:22\n",
      "... Iteration 10 ... Epoch 0.05 ... Loss 3.517 ... Acc 0.406\n",
      "... Iteration 20 ... Epoch 0.09 ... Loss 1.702 ... Acc 0.438\n",
      "... Iteration 30 ... Epoch 0.14 ... Loss 1.452 ... Acc 0.281\n",
      "... Iteration 40 ... Epoch 0.18 ... Loss 1.248 ... Acc 0.312\n",
      "... Iteration 50 ... Epoch 0.23 ... Loss 1.266 ... Acc 0.281\n",
      "... Iteration 60 ... Epoch 0.27 ... Loss 1.120 ... Acc 0.375\n",
      "... Iteration 70 ... Epoch 0.32 ... Loss 1.142 ... Acc 0.281\n",
      "... Iteration 80 ... Epoch 0.36 ... Loss 1.089 ... Acc 0.406\n",
      "... Iteration 90 ... Epoch 0.41 ... Loss 1.134 ... Acc 0.375\n",
      "... Iteration 100 ... Epoch 0.45 ... Loss 1.173 ... Acc 0.250\n",
      "... Iteration 110 ... Epoch 0.50 ... Loss 1.078 ... Acc 0.438\n",
      "... Iteration 120 ... Epoch 0.54 ... Loss 1.095 ... Acc 0.344\n",
      "... Iteration 130 ... Epoch 0.59 ... Loss 1.124 ... Acc 0.219\n",
      "... Iteration 140 ... Epoch 0.63 ... Loss 1.076 ... Acc 0.438\n",
      "... Iteration 150 ... Epoch 0.68 ... Loss 1.088 ... Acc 0.312\n",
      "... Iteration 160 ... Epoch 0.72 ... Loss 1.132 ... Acc 0.375\n",
      "... Iteration 170 ... Epoch 0.77 ... Loss 1.079 ... Acc 0.406\n",
      "... Iteration 180 ... Epoch 0.81 ... Loss 1.163 ... Acc 0.312\n",
      "... Iteration 190 ... Epoch 0.86 ... Loss 1.096 ... Acc 0.250\n",
      "... Iteration 200 ... Epoch 0.90 ... Loss 1.079 ... Acc 0.438\n",
      "... Iteration 210 ... Epoch 0.95 ... Loss 1.095 ... Acc 0.344\n",
      "... Iteration 220 ... Epoch 0.99 ... Loss 1.081 ... Acc 0.469\n",
      "Epoch 1 Starting @ 2019-11-19 06:08:41\n",
      "... Iteration 230 ... Epoch 1.04 ... Loss 1.091 ... Acc 0.438\n",
      "... Iteration 240 ... Epoch 1.08 ... Loss 1.098 ... Acc 0.406\n",
      "... Iteration 250 ... Epoch 1.13 ... Loss 1.084 ... Acc 0.375\n",
      "... Iteration 260 ... Epoch 1.17 ... Loss 1.082 ... Acc 0.281\n",
      "... Iteration 270 ... Epoch 1.22 ... Loss 1.142 ... Acc 0.250\n",
      "... Iteration 280 ... Epoch 1.26 ... Loss 1.118 ... Acc 0.344\n",
      "... Iteration 290 ... Epoch 1.31 ... Loss 1.145 ... Acc 0.281\n",
      "... Iteration 300 ... Epoch 1.35 ... Loss 1.082 ... Acc 0.469\n",
      "... Iteration 310 ... Epoch 1.40 ... Loss 1.163 ... Acc 0.312\n",
      "... Iteration 320 ... Epoch 1.44 ... Loss 1.131 ... Acc 0.312\n",
      "... Iteration 330 ... Epoch 1.49 ... Loss 1.111 ... Acc 0.406\n",
      "... Iteration 340 ... Epoch 1.53 ... Loss 1.094 ... Acc 0.312\n",
      "... Iteration 350 ... Epoch 1.58 ... Loss 1.117 ... Acc 0.375\n",
      "... Iteration 360 ... Epoch 1.62 ... Loss 1.092 ... Acc 0.469\n",
      "... Iteration 370 ... Epoch 1.67 ... Loss 1.110 ... Acc 0.250\n",
      "... Iteration 380 ... Epoch 1.71 ... Loss 1.147 ... Acc 0.281\n",
      "... Iteration 390 ... Epoch 1.76 ... Loss 1.110 ... Acc 0.281\n",
      "... Iteration 400 ... Epoch 1.80 ... Loss 1.088 ... Acc 0.344\n",
      "... Iteration 410 ... Epoch 1.85 ... Loss 1.082 ... Acc 0.438\n",
      "... Iteration 420 ... Epoch 1.89 ... Loss 1.100 ... Acc 0.344\n",
      "... Iteration 430 ... Epoch 1.94 ... Loss 1.107 ... Acc 0.250\n",
      "... Iteration 440 ... Epoch 1.98 ... Loss 1.104 ... Acc 0.344\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "engine = EngineGraph(model, config)\n",
    "engine.train()\n",
    "# engine.load_state(\"/app/GraphNets/dump/gcn20191119_055124/gcn_kipf_best_400_1.111302137374878.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing new validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating model on the validation set\n",
      "Iteration : 0, Progress 0.0 \n",
      "Iteration : 1, Progress 0.03571428571428571 \n",
      "Iteration : 2, Progress 0.07142857142857142 \n",
      "Iteration : 3, Progress 0.10714285714285714 \n",
      "Iteration : 4, Progress 0.14285714285714285 \n",
      "Iteration : 5, Progress 0.17857142857142858 \n",
      "Iteration : 6, Progress 0.21428571428571427 \n",
      "Iteration : 7, Progress 0.25 \n",
      "dumping\n",
      "Iteration : 8, Progress 0.2857142857142857 \n",
      "Iteration : 9, Progress 0.32142857142857145 \n",
      "Iteration : 10, Progress 0.35714285714285715 \n",
      "Iteration : 11, Progress 0.39285714285714285 \n",
      "Iteration : 12, Progress 0.42857142857142855 \n",
      "Iteration : 13, Progress 0.4642857142857143 \n",
      "Iteration : 14, Progress 0.5 \n",
      "Iteration : 15, Progress 0.5357142857142857 \n",
      "dumping\n",
      "Iteration : 16, Progress 0.5714285714285714 \n",
      "Iteration : 17, Progress 0.6071428571428571 \n",
      "Iteration : 18, Progress 0.6428571428571429 \n",
      "Iteration : 19, Progress 0.6785714285714286 \n",
      "Iteration : 20, Progress 0.7142857142857143 \n",
      "Iteration : 21, Progress 0.75 \n",
      "Iteration : 22, Progress 0.7857142857142857 \n",
      "Iteration : 23, Progress 0.8214285714285714 \n",
      "dumping\n",
      "Iteration : 24, Progress 0.8571428571428571 \n",
      "Iteration : 25, Progress 0.8928571428571429 \n",
      "Iteration : 26, Progress 0.9285714285714286 \n",
      "Iteration : 27, Progress 0.9642857142857143 \n",
      "Overall acc : 0.33220338983050846, Overall loss : 1.1124253273010254"
     ]
    }
   ],
   "source": [
    "engine.validate(\"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dir = os.path.join(engine.dirpath, \"valid_validation_current\")\n",
    "data = np.load(os.path.join(validation_dir, \"0.npz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['index', 'label', 'pred', 'pred_val']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['index'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['pred'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['pred_val'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29402831, 0.30380052, 0.40217122],\n",
       "       [0.29422158, 0.30299944, 0.40277895],\n",
       "       [0.29560098, 0.29721785, 0.40718117],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.30148876, 0.27071926, 0.42779192],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29479611, 0.30060562, 0.40459833],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29982418, 0.27851266, 0.42166317],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29420277, 0.30307746, 0.40271974],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29582602, 0.2962544 , 0.40791956],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.30020112, 0.27677122, 0.42302766],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29418465, 0.30315274, 0.40266266],\n",
       "       [0.29421723, 0.30301765, 0.40276518],\n",
       "       [0.29792023, 0.2871173 , 0.41496244],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29449931, 0.30184472, 0.40365595],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29567626, 0.29689601, 0.4074277 ],\n",
       "       [0.29419538, 0.30310807, 0.40269652],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29419735, 0.30310005, 0.4027026 ],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29483053, 0.30046156, 0.40470794],\n",
       "       [0.29420504, 0.30306813, 0.40272683],\n",
       "       [0.3027477 , 0.26463258, 0.43261972],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29743868, 0.28924689, 0.4133144 ],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.294186  , 0.30314714, 0.4026669 ],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29484239, 0.30041194, 0.40474576],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.30646002, 0.24546842, 0.44807154],\n",
       "       [0.2978954 , 0.2872276 , 0.414877  ],\n",
       "       [0.30008063, 0.27732942, 0.42258993],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.3055813 , 0.25019547, 0.4442232 ],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.2988134 , 0.28312057, 0.41806599],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.2973496 , 0.289639  , 0.41301143],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29420653, 0.30306193, 0.40273154],\n",
       "       [0.29496375, 0.2999033 , 0.40513295],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29568577, 0.29685369, 0.4074606 ],\n",
       "       [0.29423782, 0.30293226, 0.40283   ],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29541034, 0.29802412, 0.40656555],\n",
       "       [0.29418984, 0.30313101, 0.40267906],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29423997, 0.30292305, 0.40283692],\n",
       "       [0.29416493, 0.30323449, 0.40260056],\n",
       "       [0.29406443, 0.30365095, 0.40228465],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29419163, 0.30312383, 0.40268457],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.2942062 , 0.3030633 , 0.40273052],\n",
       "       [0.29661694, 0.29284069, 0.41054234],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29716456, 0.29045334, 0.41238216],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29419845, 0.30309531, 0.40270615],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29417914, 0.30317551, 0.40264535],\n",
       "       [0.29573733, 0.29663453, 0.40762809],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29895005, 0.28250119, 0.41854873],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29499245, 0.29978299, 0.40522462],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.2941992 , 0.30309236, 0.40270841],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29496977, 0.29987821, 0.40515208],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29420421, 0.30307162, 0.40272418],\n",
       "       [0.29610643, 0.29504895, 0.40884447],\n",
       "       [0.29341197, 0.30634025, 0.40024778],\n",
       "       [0.29849395, 0.28455651, 0.41694963],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.2948716 , 0.30028951, 0.40483886],\n",
       "       [0.29419956, 0.3030909 , 0.40270957],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29478893, 0.3006354 , 0.40457559],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29492468, 0.30006713, 0.40500817],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29738981, 0.28946206, 0.41314816],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29420671, 0.30306128, 0.40273207],\n",
       "       [0.30428353, 0.25694722, 0.43876922],\n",
       "       [0.29553065, 0.2975156 , 0.40695375],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29498798, 0.29980159, 0.40521044],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29669464, 0.29250327, 0.41080213],\n",
       "       [0.29558498, 0.2972858 , 0.4071292 ],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29420501, 0.30306828, 0.40272674],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29496583, 0.29989472, 0.40513951],\n",
       "       [0.2942116 , 0.30304104, 0.40274736],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.293722  , 0.30506539, 0.40121266],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29449815, 0.30184945, 0.40365234],\n",
       "       [0.29993516, 0.27800119, 0.42206365],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.30058458, 0.27498633, 0.42442912],\n",
       "       [0.29495862, 0.29992476, 0.40511659],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29763639, 0.28837469, 0.41398892],\n",
       "       [0.29419523, 0.30310875, 0.40269598],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29883134, 0.28303757, 0.41813102],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29494831, 0.29996818, 0.40508354],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29577425, 0.29647645, 0.40774935],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29865572, 0.28382942, 0.41751489],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29925826, 0.28110242, 0.41963929],\n",
       "       [0.29421684, 0.30301917, 0.40276402],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.30128381, 0.271694  , 0.42702225],\n",
       "       [0.29588309, 0.29601124, 0.40810561],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29421279, 0.3030358 , 0.40275133],\n",
       "       [0.29737866, 0.28951126, 0.41311017],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29420498, 0.30306825, 0.40272674],\n",
       "       [0.29655692, 0.29310128, 0.41034177],\n",
       "       [0.29556122, 0.29738647, 0.40705237],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29556102, 0.29738724, 0.40705177],\n",
       "       [0.30009955, 0.27724153, 0.42265883],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.30324063, 0.26219901, 0.43456036],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29341197, 0.30634025, 0.40024778],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29511124, 0.29928413, 0.40560466],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29637101, 0.2939069 , 0.40972212],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340449, 0.30637094, 0.40022457],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29340816, 0.30635604, 0.40023586],\n",
       "       [0.29449201, 0.30187511, 0.40363285]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['pred_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.40217122, 0.40277895, 0.40718117, 0.40022457, 0.42779192,\n",
       "       0.40023586, 0.40459833, 0.40023586, 0.42166317, 0.40023586,\n",
       "       0.40023586, 0.40023586, 0.40271974, 0.40023586, 0.40791956,\n",
       "       0.40023586, 0.40022457, 0.40023586, 0.40022457, 0.42302766,\n",
       "       0.40023586, 0.40022457, 0.40023586, 0.40023586, 0.40266266,\n",
       "       0.40276518, 0.41496244, 0.40023586, 0.40022457, 0.40022457,\n",
       "       0.40365595, 0.40023586, 0.4074277 , 0.40269652, 0.40023586,\n",
       "       0.40022457, 0.40023586, 0.40022457, 0.40022457, 0.40022457,\n",
       "       0.40022457, 0.4027026 , 0.40023586, 0.40470794, 0.40272683,\n",
       "       0.43261972, 0.40022457, 0.40023586, 0.40023586, 0.4133144 ,\n",
       "       0.40022457, 0.4026669 , 0.40022457, 0.40022457, 0.40022457,\n",
       "       0.40474576, 0.40022457, 0.44807154, 0.414877  , 0.42258993,\n",
       "       0.40023586, 0.40022457, 0.40023586, 0.4442232 , 0.40023586,\n",
       "       0.40023586, 0.40022457, 0.40022457, 0.40022457, 0.40023586,\n",
       "       0.40023586, 0.40022457, 0.40022457, 0.40022457, 0.40022457,\n",
       "       0.41806599, 0.40023586, 0.40022457, 0.40023586, 0.41301143,\n",
       "       0.40022457, 0.40022457, 0.40022457, 0.40023586, 0.40023586,\n",
       "       0.40273154, 0.40513295, 0.40023586, 0.4074606 , 0.40283   ,\n",
       "       0.40022457, 0.40023586, 0.40023586, 0.40656555, 0.40267906,\n",
       "       0.40022457, 0.40283692, 0.40260056, 0.40228465, 0.40023586,\n",
       "       0.40023586, 0.40268457, 0.40022457, 0.40022457, 0.40023586,\n",
       "       0.40023586, 0.40022457, 0.40023586, 0.40273052, 0.41054234,\n",
       "       0.40022457, 0.40022457, 0.40023586, 0.40022457, 0.41238216,\n",
       "       0.40023586, 0.40023586, 0.40023586, 0.40023586, 0.40270615,\n",
       "       0.40022457, 0.40023586, 0.40264535, 0.40762809, 0.40022457,\n",
       "       0.40023586, 0.40022457, 0.40023586, 0.41854873, 0.40022457,\n",
       "       0.40522462, 0.40023586, 0.40022457, 0.40023586, 0.40023586,\n",
       "       0.40270841, 0.40022457, 0.40515208, 0.40023586, 0.40272418,\n",
       "       0.40884447, 0.40024778, 0.41694963, 0.40022457, 0.40022457,\n",
       "       0.40483886, 0.40270957, 0.40023586, 0.40023586, 0.40457559,\n",
       "       0.40022457, 0.40500817, 0.40023586, 0.41314816, 0.40022457,\n",
       "       0.40273207, 0.43876922, 0.40695375, 0.40023586, 0.40022457,\n",
       "       0.40521044, 0.40023586, 0.40022457, 0.41080213, 0.4071292 ,\n",
       "       0.40023586, 0.40272674, 0.40022457, 0.40513951, 0.40274736,\n",
       "       0.40023586, 0.40023586, 0.40022457, 0.40023586, 0.40022457,\n",
       "       0.40121266, 0.40022457, 0.40365234, 0.42206365, 0.40023586,\n",
       "       0.40022457, 0.42442912, 0.40511659, 0.40023586, 0.40022457,\n",
       "       0.41398892, 0.40269598, 0.40022457, 0.40022457, 0.40022457,\n",
       "       0.40023586, 0.40022457, 0.41813102, 0.40022457, 0.40023586,\n",
       "       0.40508354, 0.40023586, 0.40023586, 0.40774935, 0.40023586,\n",
       "       0.41751489, 0.40023586, 0.40022457, 0.41963929, 0.40276402,\n",
       "       0.40023586, 0.40022457, 0.40023586, 0.40022457, 0.40022457,\n",
       "       0.40023586, 0.40023586, 0.40022457, 0.40022457, 0.40022457,\n",
       "       0.40023586, 0.42702225, 0.40810561, 0.40022457, 0.40275133,\n",
       "       0.41311017, 0.40023586, 0.40023586, 0.40272674, 0.41034177,\n",
       "       0.40705237, 0.40023586, 0.40023586, 0.40705177, 0.42265883,\n",
       "       0.40022457, 0.40023586, 0.40023586, 0.40022457, 0.40023586,\n",
       "       0.43456036, 0.40023586, 0.40023586, 0.40023586, 0.40023586,\n",
       "       0.40024778, 0.40023586, 0.40022457, 0.40560466, 0.40022457,\n",
       "       0.40022457, 0.40022457, 0.40972212, 0.40023586, 0.40023586,\n",
       "       0.40022457, 0.40023586, 0.40023586, 0.40023586, 0.40023586,\n",
       "       0.40363285])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['pred_val'][np.arange(256), data['pred']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
