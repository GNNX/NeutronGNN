{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/evn/ python\n",
    "\n",
    "import h5py\n",
    "import os \n",
    "import argparse\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Separate Train and Validation from Test data\")\n",
    "    parser.add_argument(\"h5_file\",\n",
    "                        type=str,\n",
    "                        help=\"Path to h5_file,\\\n",
    "                        must contain 'event_data'\")\n",
    "    parser.add_argument('output_folder', type=str,\n",
    "                        help=\"Path to output folder.\")\n",
    "    parser.add_argument('indices_folder', type=str, help=\"Path to indices folder\")\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def load_indices(indices_file):\n",
    "    with open(indices_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    # indicies = [int(l.strip()) for l in lines if not l.isspace()]\n",
    "    indices = [int(l.strip()) for l in lines]\n",
    "    return indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EasyDict(dict):\n",
    "    def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs)\n",
    "    def __getattr__(self, name): return self[name]\n",
    "    def __setattr__(self, name, value): self[name] = value\n",
    "    def __delattr__(self, name): del self[name]\n",
    "        \n",
    "config = EasyDict()\n",
    "config.h5_file = \"/app/test_data/IWCDmPMT_4pi_fulltank_test_graphnet.h5\"\n",
    "config.indices_folder = \"/app/test_data/IWCDmPMT_4pi_fulltank_test_splits/\"\n",
    "config.output_folder = \"/app/test_data/split_h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "885 7963\n"
     ]
    }
   ],
   "source": [
    "test_indices = load_indices(os.path.join(config.indices_folder, \"test.txt\"))\n",
    "train_indices = load_indices(os.path.join(config.indices_folder, \"train.txt\"))\n",
    "val_indices = load_indices(os.path.join(config.indices_folder, \"val.txt\"))\n",
    "\n",
    "test_set = set(test_indices)\n",
    "train_set = set(train_indices)\n",
    "val_set = set(val_indices)\n",
    "\n",
    "test_length = len(test_indices)\n",
    "train_length = len(train_indices) + len(val_indices)\n",
    "\n",
    "print(test_length, train_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IWCDmPMT_4pi_fulltank_test_graphnet_test.h5 IWCDmPMT_4pi_fulltank_test_graphnet_trainval.h5\n"
     ]
    }
   ],
   "source": [
    "basename, extension = os.path.splitext(os.path.basename(config.h5_file))\n",
    "test_filename = basename + \"_test\" + extension\n",
    "train_filename = basename + \"_trainval\" + extension\n",
    "\n",
    "print(test_filename, train_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/app/test_data/split_h5/IWCDmPMT_4pi_fulltank_test_graphnet_test.h5 /app/test_data/split_h5/IWCDmPMT_4pi_fulltank_test_graphnet_trainval.h5\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(config.output_folder, exist_ok=True)\n",
    "\n",
    "test_filepath = os.path.join(config.output_folder, test_filename)\n",
    "train_filepath = os.path.join(config.output_folder, train_filename)\n",
    "\n",
    "print(test_filepath, train_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angles\n",
      "energies\n",
      "event_data\n",
      "event_ids\n",
      "labels\n",
      "positions\n"
     ]
    }
   ],
   "source": [
    "# Read in original file\n",
    "with h5py.File(config.h5_file, 'r') as infile:\n",
    "    keys = list(infile.keys())\n",
    "\n",
    "    # Writing both file at the same time for sequential read\n",
    "    with h5py.File(test_filepath, 'w') as testfile:\n",
    "        with h5py.File(train_filepath, 'w') as trainfile:\n",
    "            for key in keys:\n",
    "                if key == \"root_files\":\n",
    "                    continue\n",
    "                print(key)\n",
    "                # Get info for original data\n",
    "                original_data = infile[key]\n",
    "                original_shape = original_data.shape\n",
    "                original_dtype = original_data.dtype\n",
    "\n",
    "                zero = np.zeros(original_shape[1:], dtype=original_dtype)\n",
    "                \n",
    "                # Pre initialize test data to get offset\n",
    "                test_shape = (test_length,) + original_shape[1:]\n",
    "                test_data = testfile.create_dataset(key, shape=test_shape, \n",
    "                                                    dtype=original_dtype, fillvalue=0)\n",
    "#                 test_data[:] = np.zeros(test_shape).astype(original_dtype)\n",
    "                test_data[:] = zero\n",
    "#                 for i in range(test_length):\n",
    "#                     test_data[i] = zero\n",
    "                \n",
    "                # Pre initialize train data to get offset\n",
    "                train_shape = (train_length,) + original_shape[1:]\n",
    "                train_data = trainfile.create_dataset(key, shape=train_shape,\n",
    "                                                    dtype=original_dtype, fillvalue=0)\n",
    "#                 train_data[:] = np.zeros(train_shape).astype(original_dtype)\n",
    "                train_data[:] = zero\n",
    "#                 for i in range(train_length):\n",
    "#                     train_data[i] = zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angles\n",
      "1020680\n",
      "2048\n",
      "2048\n",
      "energies\n",
      "843720\n",
      "9128\n",
      "65752\n",
      "event_data\n",
      "1091464\n",
      "12668\n",
      "97604\n",
      "event_ids\n",
      "3208\n",
      "111935356\n",
      "1007132484\n",
      "labels\n",
      "985288\n",
      "111938896\n",
      "1007164336\n",
      "positions\n",
      "879112\n",
      "111942436\n",
      "1007196188\n"
     ]
    }
   ],
   "source": [
    "# Read in original file\n",
    "with h5py.File(config.h5_file, 'r') as infile:\n",
    "    keys = list(infile.keys())\n",
    "\n",
    "    # Writing both file at the same time for sequential read\n",
    "    with h5py.File(test_filepath, 'r') as testfile:\n",
    "        with h5py.File(train_filepath, 'r') as trainfile:\n",
    "            for key in keys:\n",
    "                if key == \"root_files\":\n",
    "                    continue\n",
    "                print(key)\n",
    "                # Get info for original data\n",
    "                original_data = infile[key]\n",
    "                original_shape = original_data.shape\n",
    "                original_dtype = original_data.dtype\n",
    "\n",
    "                # Pre initialize test data to get offset\n",
    "                test_data = testfile[key]\n",
    "                test_shape = test_data.shape\n",
    "                \n",
    "                # Pre initialize train data to get offset\n",
    "                train_data = trainfile[key]\n",
    "                train_shape = train_data.shape\n",
    "                \n",
    "                # Get offset\n",
    "                original_offset = original_data.id.get_offset()\n",
    "                test_offset = test_data.id.get_offset()\n",
    "                train_offset = train_data.id.get_offset()\n",
    "                \n",
    "                print(original_offset)\n",
    "                print(test_offset)\n",
    "                print(train_offset)\n",
    "                \n",
    "                # Setup mem data\n",
    "                original_mem_data = np.memmap(config.h5_file, mode='r', shape=original_shape,\n",
    "                                                offset=original_offset, dtype=original_dtype)\n",
    "                test_mem_data = np.memmap(test_filepath, mode='readwrite', shape=test_shape,\n",
    "                                            offset=test_offset, dtype=original_dtype)\n",
    "                train_mem_data = np.memmap(train_filepath, mode='readwrite', shape=train_shape,\n",
    "                                            offset=train_offset, dtype=original_dtype)\n",
    "\n",
    "                # Copy\n",
    "                test_i = 0\n",
    "                train_i = 0\n",
    "                for i, data in enumerate(original_mem_data):\n",
    "                    if i in test_set:\n",
    "                        test_mem_data[test_i] = data\n",
    "                        test_i += 1\n",
    "                    else:\n",
    "                        train_mem_data[train_i] = data\n",
    "                        train_i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_indices = []\n",
    "new_val_indices = []\n",
    "\n",
    "\n",
    "# Read in original file\n",
    "with h5py.File(config.h5_file, 'r') as infile:\n",
    "    keys = list(infile.keys())\n",
    "        \n",
    "    train_i = 0\n",
    "    for i in range(infile[keys[0]].shape[0]):\n",
    "        if i in train_set:\n",
    "            new_train_indices.append(train_i)\n",
    "            train_i += 1\n",
    "        elif i in val_set:\n",
    "            new_val_indices.append(train_i)\n",
    "            train_i += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing new indices to /app/test_data/split_h5/IWCDmPMT_4pi_fulltank_test_graphnet_splits\n"
     ]
    }
   ],
   "source": [
    "# Write new train and val indices for the new file\n",
    "splits_dir = os.path.join(config.output_folder, basename + \"_splits\")\n",
    "os.makedirs(splits_dir, exist_ok=True)\n",
    "\n",
    "print(\"Writing new indices to {}\".format(splits_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(splits_dir, 'train.txt'), 'w') as f:\n",
    "    indices = np.random.permutation(new_train_indices)\n",
    "    f.writelines([\"{}\\n\".format(i) for i in indices])\n",
    "\n",
    "with open(os.path.join(splits_dir, 'val.txt'), 'w') as f:\n",
    "    indices = np.random.permutation(new_val_indices)\n",
    "    f.writelines([\"{}\\n\".format(i) for i in indices])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "config = EasyDict()\n",
    "config.h5_file = \"/app/test_data/IWCDmPMT_4pi_fulltank_test_graphnet.h5\"\n",
    "config.indices_folder = \"/app/test_data/IWCDmPMT_4pi_fulltank_test_splits/\"\n",
    "config.output_folder = \"/app/test_data/split_h5_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "885 7963\n"
     ]
    }
   ],
   "source": [
    "test_indices = load_indices(os.path.join(config.indices_folder, \"test.txt\"))\n",
    "train_indices = load_indices(os.path.join(config.indices_folder, \"train.txt\"))\n",
    "val_indices = load_indices(os.path.join(config.indices_folder, \"val.txt\"))\n",
    "\n",
    "test_set = set(test_indices)\n",
    "train_set = set(train_indices)\n",
    "val_set = set(val_indices)\n",
    "\n",
    "test_length = len(test_indices)\n",
    "train_length = len(train_indices) + len(val_indices)\n",
    "\n",
    "print(test_length, train_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IWCDmPMT_4pi_fulltank_test_graphnet_test.h5 IWCDmPMT_4pi_fulltank_test_graphnet_trainval.h5\n"
     ]
    }
   ],
   "source": [
    "basename, extension = os.path.splitext(os.path.basename(config.h5_file))\n",
    "test_filename = basename + \"_test\" + extension\n",
    "train_filename = basename + \"_trainval\" + extension\n",
    "\n",
    "print(test_filename, train_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/app/test_data/split_h5_2/IWCDmPMT_4pi_fulltank_test_graphnet_test.h5 /app/test_data/split_h5_2/IWCDmPMT_4pi_fulltank_test_graphnet_trainval.h5\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(config.output_folder, exist_ok=True)\n",
    "\n",
    "test_filepath = os.path.join(config.output_folder, test_filename)\n",
    "train_filepath = os.path.join(config.output_folder, train_filename)\n",
    "\n",
    "print(test_filepath, train_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angles\n",
      "energies\n",
      "event_data\n",
      "event_ids\n",
      "labels\n",
      "positions\n"
     ]
    }
   ],
   "source": [
    "# Read in original file\n",
    "with h5py.File(config.h5_file, 'r') as infile:\n",
    "    keys = list(infile.keys())\n",
    "\n",
    "    # Writing both file at the same time for sequential read\n",
    "    with h5py.File(test_filepath, 'w') as testfile:\n",
    "        with h5py.File(train_filepath, 'w') as trainfile:\n",
    "            for key in keys:\n",
    "                if key == \"root_files\":\n",
    "                    continue\n",
    "                print(key)\n",
    "                # Get info for original data\n",
    "                original_data = infile[key]\n",
    "                original_shape = original_data.shape\n",
    "                original_dtype = original_data.dtype\n",
    "\n",
    "                # set up test data \n",
    "                test_shape = (test_length,) + original_shape[1:]\n",
    "                test_data = testfile.create_dataset(key, shape=test_shape, \n",
    "                                                    dtype=original_dtype)\n",
    "                \n",
    "                # set up train data\n",
    "                train_shape = (train_length,) + original_shape[1:]\n",
    "                train_data = trainfile.create_dataset(key, shape=train_shape,\n",
    "                                                    dtype=original_dtype)\n",
    "\n",
    "                # Copy\n",
    "                test_i = 0\n",
    "                train_i = 0\n",
    "                for i, data in enumerate(original_data):\n",
    "                    if i in test_set:\n",
    "                        test_data[test_i] = data\n",
    "                        test_i += 1\n",
    "                    else:\n",
    "                        train_data[train_i] = data\n",
    "                        train_i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare output result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angles\n",
      "[ 2.9884098 -2.2890291]\n",
      "[ 2.9884098 -2.2890291]\n",
      "energies\n",
      "[992.81683]\n",
      "[992.81683]\n",
      "event_data\n",
      "[[9.735686e-01 1.005100e+03]\n",
      " [0.000000e+00 0.000000e+00]\n",
      " [0.000000e+00 0.000000e+00]\n",
      " ...\n",
      " [0.000000e+00 0.000000e+00]\n",
      " [0.000000e+00 0.000000e+00]\n",
      " [0.000000e+00 0.000000e+00]]\n",
      "[[9.735686e-01 1.005100e+03]\n",
      " [0.000000e+00 0.000000e+00]\n",
      " [0.000000e+00 0.000000e+00]\n",
      " ...\n",
      " [0.000000e+00 0.000000e+00]\n",
      " [0.000000e+00 0.000000e+00]\n",
      " [0.000000e+00 0.000000e+00]]\n",
      "event_ids\n",
      "134\n",
      "134\n",
      "labels\n",
      "2\n",
      "2\n",
      "positions\n",
      "[[ 128.78215 -293.06497  319.7137 ]]\n",
      "[[ 128.78215 -293.06497  319.7137 ]]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(\"/app/test_data/split_h5/IWCDmPMT_4pi_fulltank_test_graphnet_test.h5\", 'r') as memfile:\n",
    "    with h5py.File(\"/app/test_data/split_h5_2/IWCDmPMT_4pi_fulltank_test_graphnet_test.h5\", 'r') as file:\n",
    "        for key in list(file.keys()):\n",
    "            print(key)\n",
    "            print(memfile[key][13])\n",
    "            print(file[key][13])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
